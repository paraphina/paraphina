"""
cli.py

CLI for Phase B: Confidence-Aware Statistical Gating.

Usage:
    python3 -m batch_runs.phase_b.cli \
        --candidate-run <path> \
        --baseline-run <path> \
        --out-dir <path> \
        --alpha 0.05

Inputs:
    --candidate-run and --baseline-run must point to either:
    - A Phase A run directory containing trials.jsonl
    - A direct path to a .jsonl file with trial records
    
    Phase B reads trials.jsonl from each run directory to extract
    per-trial metrics (pnl, kill_ci, etc.) for statistical gating.

Outputs:
    - confidence_report.json
    - confidence_report.md

Decision outcomes (tri-state model):
    PROMOTE: Guardrails pass AND candidate is provably better than baseline
    HOLD: Guardrails pass BUT candidate is not provably better (needs more data)
    REJECT: Guardrails fail (candidate is provably worse)
    ERROR: Missing/invalid run data, parse errors, etc.

Exit codes (institutional CI semantics):
    0 = PROMOTE (candidate is provably better) or HOLD (pipeline succeeded; not enough evidence yet)
    2 = REJECT (candidate is worse / fails guardrails)
    3 = ERROR (runtime/IO/parsing failure)
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import List, Optional

from batch_runs.phase_b.gate import (
    PromotionDecision,
    PromotionGate,
    PromotionOutcome,
    evaluate_promotion,
)
from batch_runs.phase_b.run_data import (
    load_run_data,
    RunDataError,
    TrialsFileNotFoundError,
    NoUsableObservationsError,
)


# =============================================================================
# Error Handling
# =============================================================================

def write_error_report(
    out_dir: Path,
    error_message: str,
    candidate_run: Path,
    baseline_run: Optional[Path],
    alpha: float,
    n_bootstrap: int,
    seed: int,
) -> None:
    """
    Write an error report when data loading fails.
    
    Args:
        out_dir: Output directory for reports
        error_message: Detailed error message
        candidate_run: Path to candidate run
        baseline_run: Path to baseline run (if any)
        alpha: Significance level
        n_bootstrap: Number of bootstrap samples
        seed: Random seed
    """
    from datetime import datetime, timezone
    
    out_dir.mkdir(parents=True, exist_ok=True)
    
    report = {
        "decision": "ERROR",
        "status": "error",
        "reason": error_message,
        "outcome": "error",
        "guardrails_passed": False,
        "promotion_passed": False,
        "decision_reason": f"ERROR: {error_message}",
        "is_promote": False,
        "exit_code": 3,
        "candidate_path": str(candidate_run),
        "baseline_path": str(baseline_run) if baseline_run else None,
        "candidate_metrics": None,
        "baseline_metrics": None,
        "candidate_samples": 0,
        "baseline_samples": 0,
        "comparison": None,
        "guardrail_checks": [],
        "promotion_checks": [],
        "pass_reasons": [],
        "fail_reasons": [],
        "error_message": error_message,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "alpha": alpha,
        "n_bootstrap": n_bootstrap,
        "seed": seed,
    }
    
    # Write JSON report
    json_path = out_dir / "confidence_report.json"
    with open(json_path, "w") as f:
        json.dump(report, f, indent=2)
    
    # Write Markdown report
    md_path = out_dir / "confidence_report.md"
    md_content = f"""# Phase B Confidence Gate Report

**Decision:** ERROR
**Timestamp:** {report["timestamp"]}

## Decision Rationale

> ERROR: {error_message}

- Guardrails passed: ✗ No
- Promotion criteria passed: ✗ No

## Error

```
{error_message}
```

## Configuration

- Alpha: {alpha}
- Bootstrap samples: {n_bootstrap}
- Seed: {seed}
- Candidate path: `{candidate_run}`
- Baseline path: `{baseline_run if baseline_run else "None"}`

---
*Generated by Phase B Confidence Gate*
"""
    with open(md_path, "w") as f:
        f.write(md_content)


# =============================================================================
# CLI Implementation
# =============================================================================

def run_gate(
    candidate_run: Path,
    baseline_run: Optional[Path],
    out_dir: Path,
    alpha: float = 0.05,
    n_bootstrap: int = 1000,
    seed: int = 42,
    kill_threshold: Optional[float] = None,
    drawdown_threshold: Optional[float] = None,
    pnl_threshold: Optional[float] = None,
    require_strict_dominance: bool = True,
    verbose: bool = True,
) -> PromotionDecision:
    """
    Run the Phase B confidence gate.
    
    Args:
        candidate_run: Path to candidate run directory
        baseline_run: Path to baseline run directory (optional)
        out_dir: Output directory for reports
        alpha: Significance level for confidence intervals
        n_bootstrap: Number of bootstrap samples
        seed: Random seed for reproducibility
        kill_threshold: Maximum allowed kill rate
        drawdown_threshold: Maximum allowed drawdown
        pnl_threshold: Minimum required PnL lower CI
        require_strict_dominance: Require strict dominance over baseline
        verbose: Print progress to stdout
        
    Returns:
        PromotionDecision with outcome and explanation
    """
    if verbose:
        print("=" * 70)
        print("Phase B: Confidence-Aware Statistical Gating")
        print("=" * 70)
        print(f"  Candidate: {candidate_run}")
        if baseline_run:
            print(f"  Baseline: {baseline_run}")
        print(f"  Output: {out_dir}")
        print(f"  Alpha: {alpha}")
        print(f"  Bootstrap samples: {n_bootstrap}")
        print(f"  Seed: {seed}")
        if kill_threshold is not None:
            print(f"  Kill threshold: {kill_threshold}")
        if drawdown_threshold is not None:
            print(f"  Drawdown threshold: {drawdown_threshold}")
        if pnl_threshold is not None:
            print(f"  PnL threshold: {pnl_threshold}")
        print()
    
    # Ensure output directory exists
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # Run the gate
    decision = evaluate_promotion(
        candidate_dir=candidate_run,
        baseline_dir=baseline_run,
        alpha=alpha,
        n_bootstrap=n_bootstrap,
        seed=seed,
        kill_threshold=kill_threshold,
        drawdown_threshold=drawdown_threshold,
        pnl_threshold=pnl_threshold,
        require_strict_dominance=require_strict_dominance,
    )
    
    # Write JSON report with sample counts
    json_path = out_dir / "confidence_report.json"
    report_dict = decision.to_dict()
    # Add sample counts for transparency
    if decision.candidate_metrics:
        report_dict["candidate_samples"] = decision.candidate_metrics.n_runs
    if decision.baseline_metrics:
        report_dict["baseline_samples"] = decision.baseline_metrics.n_runs
    with open(json_path, "w") as f:
        json.dump(report_dict, f, indent=2, default=str)
    
    if verbose:
        print(f"  ✓ Wrote {json_path}")
    
    # Write Markdown report
    md_path = out_dir / "confidence_report.md"
    with open(md_path, "w") as f:
        f.write(decision.to_markdown())
    
    if verbose:
        print(f"  ✓ Wrote {md_path}")
    
    # Print summary
    if verbose:
        print()
        print(decision.summary())
    
    return decision


def main(argv: Optional[List[str]] = None) -> int:
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        prog="python3 -m batch_runs.phase_b.cli",
        description="Phase B: Confidence-Aware Statistical Gating for Promotion Decisions",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Compare candidate against baseline
  python3 -m batch_runs.phase_b.cli \\
      --candidate-run runs/phaseA/study_001/trial_0001 \\
      --baseline-run runs/phaseA/study_001/trial_0000 \\
      --out-dir runs/phaseA/study_001/confidence_gate

  # Evaluate candidate with thresholds (no baseline)
  python3 -m batch_runs.phase_b.cli \\
      --candidate-run runs/phaseA/study_001/trial_0001 \\
      --out-dir runs/phaseA/study_001/confidence_gate \\
      --kill-threshold 0.10 \\
      --pnl-threshold 10.0

  # Custom confidence level
  python3 -m batch_runs.phase_b.cli \\
      --candidate-run runs/candidate \\
      --baseline-run runs/baseline \\
      --out-dir runs/gate_output \\
      --alpha 0.01 \\
      --n-bootstrap 2000

Exit codes (institutional CI semantics):
  0 = PROMOTE or HOLD (pipeline succeeded; HOLD means not enough evidence yet)
  2 = REJECT (candidate is worse / fails guardrails)
  3 = ERROR (runtime/IO/parsing failure, data errors)

Outputs:
  - confidence_report.json: Machine-readable report
  - confidence_report.md: Human-readable Markdown report
""",
    )
    
    # Required arguments
    parser.add_argument(
        "--candidate-run",
        type=Path,
        required=True,
        help="Path to candidate run directory",
    )
    
    # Optional baseline
    parser.add_argument(
        "--baseline-run",
        type=Path,
        default=None,
        help="Path to baseline run directory (optional)",
    )
    
    # Output directory
    parser.add_argument(
        "--out-dir",
        type=Path,
        required=True,
        help="Output directory for reports",
    )
    
    # Statistical parameters
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.05,
        help="Significance level for confidence intervals (default: 0.05)",
    )
    
    parser.add_argument(
        "--n-bootstrap",
        type=int,
        default=1000,
        help="Number of bootstrap samples (default: 1000)",
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility (default: 42)",
    )
    
    # Thresholds
    parser.add_argument(
        "--kill-threshold",
        type=float,
        default=None,
        help="Maximum allowed kill rate (optional)",
    )
    
    parser.add_argument(
        "--drawdown-threshold",
        type=float,
        default=None,
        help="Maximum allowed drawdown upper CI (optional)",
    )
    
    parser.add_argument(
        "--pnl-threshold",
        type=float,
        default=None,
        help="Minimum required PnL lower CI (optional)",
    )
    
    # Comparison mode
    parser.add_argument(
        "--no-strict-dominance",
        action="store_true",
        help="Don't require strict dominance (only non-inferiority)",
    )
    
    # Verbosity
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress verbose output",
    )
    
    args = parser.parse_args(argv)
    
    # Validate paths
    if not args.candidate_run.exists():
        print(f"ERROR: Candidate run directory not found: {args.candidate_run}", file=sys.stderr)
        return 3
    
    if args.baseline_run is not None and not args.baseline_run.exists():
        print(f"ERROR: Baseline run directory not found: {args.baseline_run}", file=sys.stderr)
        return 3
    
    # Validate run data can be loaded before running gate
    try:
        # Pre-validate candidate
        candidate_data = load_run_data(args.candidate_run)
        if not args.quiet:
            print(f"  Candidate trials: {candidate_data.n_observations} observations from {candidate_data.trials_file}")
        
        # Pre-validate baseline if provided
        baseline_data = None
        if args.baseline_run is not None:
            baseline_data = load_run_data(args.baseline_run)
            if not args.quiet:
                print(f"  Baseline trials: {baseline_data.n_observations} observations from {baseline_data.trials_file}")
        
        if not args.quiet:
            print()
            
    except RunDataError as e:
        # Data loading failed - write error report and exit with code 3
        error_msg = str(e)
        print(f"ERROR: {error_msg}", file=sys.stderr)
        
        # Ensure output directory exists and write error report
        args.out_dir.mkdir(parents=True, exist_ok=True)
        write_error_report(
            out_dir=args.out_dir,
            error_message=error_msg,
            candidate_run=args.candidate_run,
            baseline_run=args.baseline_run,
            alpha=args.alpha,
            n_bootstrap=args.n_bootstrap,
            seed=args.seed,
        )
        
        if not args.quiet:
            print(f"  ✓ Wrote {args.out_dir / 'confidence_report.json'}")
            print(f"  ✓ Wrote {args.out_dir / 'confidence_report.md'}")
        
        return 3
    
    # Run the gate
    try:
        decision = run_gate(
            candidate_run=args.candidate_run,
            baseline_run=args.baseline_run,
            out_dir=args.out_dir,
            alpha=args.alpha,
            n_bootstrap=args.n_bootstrap,
            seed=args.seed,
            kill_threshold=args.kill_threshold,
            drawdown_threshold=args.drawdown_threshold,
            pnl_threshold=args.pnl_threshold,
            require_strict_dominance=not args.no_strict_dominance,
            verbose=not args.quiet,
        )
        return decision.exit_code
    except RunDataError as e:
        # This shouldn't happen since we pre-validated, but handle it anyway
        error_msg = str(e)
        print(f"ERROR: {error_msg}", file=sys.stderr)
        write_error_report(
            out_dir=args.out_dir,
            error_message=error_msg,
            candidate_run=args.candidate_run,
            baseline_run=args.baseline_run,
            alpha=args.alpha,
            n_bootstrap=args.n_bootstrap,
            seed=args.seed,
        )
        return 3
    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 3


if __name__ == "__main__":
    sys.exit(main())

