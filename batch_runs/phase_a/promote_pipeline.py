#!/usr/bin/env python3
"""
promote_pipeline.py

Phase A-2: Multi-objective tuning + promotion pipeline.

This module implements a deterministic, budget-gated research→promotion loop
using existing Rust binaries (monte_carlo + sim_eval) and suite YAMLs.

Usage:
    python3 -m batch_runs.phase_a.promote_pipeline --help
    python3 -m batch_runs.phase_a.promote_pipeline --smoke --study-dir runs/phaseA_smoke
    python3 -m batch_runs.phase_a.promote_pipeline --trials 10 --study my_study

    # With ADR (Adversarial Delta Regression) gating:
    python3 -m batch_runs.phase_a.promote_pipeline --smoke --adr-enable --study-dir runs/phaseA_smoke_adr

The pipeline:
1. Generates candidate configurations (seeded RNG + optional mutation)
2. For each candidate:
   - Creates isolated trial directory: runs/phaseA/<study>/<trial_id>/
   - Writes candidate.env with config overrides
   - Runs monte_carlo to <trial>/mc/
   - Verifies evidence pack
   - Runs out-of-sample suite (research_v1.yaml)
   - Runs adversarial regression suite (adversarial_regression_v1.yaml)
   - Parses metrics from mc_summary.json
   - [ADR] If enabled: generates baseline-vs-candidate reports via sim_eval report
3. Computes Pareto frontier
4. Selects winners per budget tier
5. Promotes candidates that pass budget + suite gates (+ ADR gates if enabled)
6. Outputs: trials.jsonl, pareto.json, pareto.csv, promoted presets

Non-negotiable constraints:
- Deterministic: fixed seeds => stable results
- Stable ordering for JSON/CSV output
- Parse JSON artifacts (mc_summary.json, run_summary.json) - not stdout
- Stdlib + optional PyYAML only (no pandas)
"""

from __future__ import annotations

import argparse
import hashlib
import json
import math
import os
import random
import subprocess
import sys
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Set

# Import confidence bound functions from stats module
from batch_runs.phase_a.stats import (
    wilson_ucb,
    normal_lcb,
    get_statistics_metadata,
)

# Phase B imports (optional - fail gracefully if not available)
try:
    from batch_runs.phase_b.gate import (
        PromotionDecision as PhaseBDecision,
        PromotionGate as PhaseBGate,
        PromotionOutcome as PhaseBOutcome,
    )
    PHASE_B_AVAILABLE = True
except ImportError:
    PHASE_B_AVAILABLE = False

# ===========================================================================
# Paths
# ===========================================================================

ROOT = Path(__file__).resolve().parents[2]
MONTE_CARLO_BIN = ROOT / "target" / "release" / "monte_carlo"
SIM_EVAL_BIN = ROOT / "target" / "release" / "sim_eval"

RESEARCH_SUITE = ROOT / "scenarios" / "suites" / "research_v1.yaml"
ADVERSARIAL_SUITE_V2 = ROOT / "scenarios" / "suites" / "adversarial_regression_v2.yaml"
ADVERSARIAL_SUITE_V1 = ROOT / "scenarios" / "suites" / "adversarial_regression_v1.yaml"

# Scenario Library v1 paths (promotion-critical)
SCENARIO_LIBRARY_FULL_SUITE = ROOT / "scenarios" / "suites" / "scenario_library_v1.yaml"
SCENARIO_LIBRARY_SMOKE_SUITE = ROOT / "scenarios" / "suites" / "scenario_library_smoke_v1.yaml"
SCENARIO_LIBRARY_DIR = ROOT / "scenarios" / "v1" / "scenario_library_v1"
SCENARIO_LIBRARY_MANIFEST = SCENARIO_LIBRARY_DIR / "manifest_sha256.json"


def get_adversarial_suite() -> Tuple[Path, str]:
    """
    Get the adversarial regression suite path.
    
    Prefers v2 (path-based scenarios) if available, falls back to v1.
    v2 is generated by adversarial_search_promote.py and uses path-based
    scenarios only (no inline env_overrides).
    
    Returns: (suite_path, version_string)
    """
    if ADVERSARIAL_SUITE_V2.exists():
        return ADVERSARIAL_SUITE_V2, "v2"
    return ADVERSARIAL_SUITE_V1, "v1"


def get_scenario_library_suite(smoke: bool, override_path: Optional[Path] = None) -> Path:
    """
    Get the scenario library suite path.
    
    Args:
        smoke: If True, use the smoke suite (5 scenarios); else use full suite (10 scenarios)
        override_path: Optional path override for custom suite
    
    Returns: Path to the scenario library suite
    """
    if override_path is not None:
        return override_path
    if smoke:
        return SCENARIO_LIBRARY_SMOKE_SUITE
    return SCENARIO_LIBRARY_FULL_SUITE


# ===========================================================================
# Scenario Library Integrity Check
# ===========================================================================

def check_scenario_library_integrity(verbose: bool = True) -> Tuple[bool, List[str]]:
    """
    Verify scenario library manifest integrity.
    
    Recomputes SHA-256 hashes of all files in the scenario library and
    validates them against the manifest. This ensures no tampering or
    drift has occurred.
    
    Args:
        verbose: Print progress messages
    
    Returns:
        (success, list of error messages)
    """
    errors: List[str] = []
    
    if not SCENARIO_LIBRARY_MANIFEST.exists():
        errors.append(f"Scenario library manifest not found: {SCENARIO_LIBRARY_MANIFEST}")
        errors.append("Run: python3 -m batch_runs.phase_a.scenario_library_v1 generate --seed <seed>")
        return False, errors
    
    try:
        with open(SCENARIO_LIBRARY_MANIFEST, "r", encoding="utf-8") as f:
            manifest = json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        errors.append(f"Failed to read manifest: {e}")
        return False, errors
    
    # Check schema version
    if manifest.get("schema_version") != 1:
        errors.append(f"Unknown manifest schema_version: {manifest.get('schema_version')}")
    
    # Check all files
    expected_files = manifest.get("files", {})
    
    for filename, expected_hash in sorted(expected_files.items()):
        filepath = SCENARIO_LIBRARY_DIR / filename
        
        if not filepath.exists():
            errors.append(f"Missing file: {filename}")
            continue
        
        # Compute hash
        h = hashlib.sha256()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(65536), b""):
                h.update(chunk)
        actual_hash = h.hexdigest()
        
        if actual_hash != expected_hash:
            errors.append(
                f"Hash mismatch for {filename}: "
                f"expected {expected_hash[:16]}..., "
                f"got {actual_hash[:16]}..."
            )
    
    # Check for extra files (excluding manifest)
    actual_files = set()
    for p in SCENARIO_LIBRARY_DIR.iterdir():
        if p.is_file() and p.name != "manifest_sha256.json":
            actual_files.add(p.name)
    
    manifest_files = set(expected_files.keys())
    extra = actual_files - manifest_files
    if extra:
        errors.append(f"Extra files not in manifest: {sorted(extra)}")
    
    if verbose:
        if errors:
            print(f"Scenario library integrity check FAILED ({len(errors)} errors):")
            for e in errors:
                print(f"  - {e}")
        else:
            print(f"Scenario library integrity check PASSED ({len(expected_files)} files verified)")
    
    return len(errors) == 0, errors


def validate_scenario_library_suite(suite_path: Path, verbose: bool = True) -> Tuple[bool, List[str]]:
    """
    Validate that a scenario library suite file exists and references valid scenarios.
    
    Args:
        suite_path: Path to the suite YAML file
        verbose: Print progress messages
    
    Returns:
        (success, list of error messages)
    """
    errors: List[str] = []
    
    if not suite_path.exists():
        errors.append(f"Suite file not found: {suite_path}")
        return False, errors
    
    # Read suite content
    try:
        content = suite_path.read_text()
    except IOError as e:
        errors.append(f"Failed to read suite file: {e}")
        return False, errors
    
    # Find scenario paths (simple line-based parsing)
    scenario_count = 0
    for line in content.split("\n"):
        line = line.strip()
        if line.startswith("- path:"):
            path_str = line.split(":", 1)[1].strip()
            full_path = ROOT / path_str
            if not full_path.exists():
                errors.append(f"Referenced scenario not found: {path_str}")
            scenario_count += 1
    
    if scenario_count == 0:
        errors.append(f"Suite has 0 scenarios: {suite_path}")
    
    if verbose:
        if errors:
            print(f"Suite validation FAILED ({len(errors)} errors):")
            for e in errors:
                print(f"  - {e}")
        else:
            print(f"Suite validation PASSED ({scenario_count} scenarios)")
    
    return len(errors) == 0, errors


DEFAULT_RUNS_DIR = ROOT / "runs"
DEFAULT_BUDGETS_FILE = Path(__file__).parent / "budgets.yaml"
PROMOTED_DIR = ROOT / "configs" / "presets" / "promoted"


# ===========================================================================
# Evidence Pack Helpers
# ===========================================================================

def _write_evidence_pack(out_dir: Path, verbose: bool = True, smoke: bool = False) -> int:
    """
    Write a root evidence pack for the output directory.
    
    Calls `sim_eval write-evidence-pack <out_dir>`.
    
    Returns exit code (0 = success).
    """
    if SIM_EVAL_BIN.exists():
        cmd = [str(SIM_EVAL_BIN), "write-evidence-pack", str(out_dir)]
    else:
        cmd = ["cargo", "run", "-p", "paraphina", "--bin", "sim_eval",
               "--release" if not smoke else "",
               "--", "write-evidence-pack", str(out_dir)]
        cmd = [c for c in cmd if c]
    
    try:
        proc = subprocess.run(
            cmd,
            cwd=str(ROOT),
            capture_output=True,
            text=True,
        )
        if verbose and proc.returncode == 0:
            for line in proc.stdout.strip().split('\n'):
                if line:
                    print(f"    {line}")
        elif proc.returncode != 0:
            print(f"    ERROR: {proc.stderr.strip()}")
        return proc.returncode
    except Exception as e:
        print(f"    ERROR: Failed to run sim_eval write-evidence-pack: {e}")
        return 1


def _verify_evidence_pack(out_dir: Path, verbose: bool = True) -> int:
    """
    Verify a root evidence pack.
    
    Calls `sim_eval verify-evidence-pack <out_dir>`.
    
    Returns exit code (0 = success).
    """
    if SIM_EVAL_BIN.exists():
        cmd = [str(SIM_EVAL_BIN), "verify-evidence-pack", str(out_dir)]
    else:
        cmd = ["cargo", "run", "-p", "paraphina", "--bin", "sim_eval",
               "--", "verify-evidence-pack", str(out_dir)]
    
    try:
        proc = subprocess.run(
            cmd,
            cwd=str(ROOT),
            capture_output=True,
            text=True,
        )
        if verbose and proc.returncode == 0:
            for line in proc.stdout.strip().split('\n'):
                if line:
                    print(f"    {line}")
        elif proc.returncode != 0:
            print(f"    ERROR: {proc.stderr.strip()}")
        return proc.returncode
    except Exception as e:
        print(f"    ERROR: Failed to run sim_eval verify-evidence-pack: {e}")
        return 1


# ===========================================================================
# Scenario Library Configuration
# ===========================================================================

@dataclass
class ScenarioLibraryConfig:
    """Configuration for Scenario Library v1 integration."""
    enabled: bool = True  # Scenario library is INCLUDED BY DEFAULT
    suite_path: Optional[Path] = None  # Override suite path; None uses default
    smoke_mode: bool = False  # If True, use smoke suite; else use full suite
    
    def get_suite_path(self) -> Path:
        """Get the effective suite path."""
        return get_scenario_library_suite(self.smoke_mode, self.suite_path)


@dataclass
class ScenarioLibraryResult:
    """Result from running scenario library suite."""
    ran: bool = False
    skipped: bool = False
    skip_reason: Optional[str] = None
    suite_path: Optional[str] = None
    output_dir: Optional[str] = None
    passed: bool = False
    errors: List[str] = field(default_factory=list)
    evidence_verified: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "ran": self.ran,
            "skipped": self.skipped,
            "skip_reason": self.skip_reason,
            "suite_path": self.suite_path,
            "output_dir": self.output_dir,
            "passed": self.passed,
            "errors": self.errors,
            "evidence_verified": self.evidence_verified,
        }


# ===========================================================================
# ADR (Adversarial Delta Regression) Configuration
# ===========================================================================

@dataclass
class ADRConfig:
    """Configuration for Adversarial Delta Regression gating."""
    enabled: bool = False
    suites: List[Path] = field(default_factory=list)
    baseline_cache_dir: Optional[Path] = None
    gate_max_regression_usd: Optional[float] = None
    gate_max_regression_pct: Optional[float] = None
    write_md: bool = True
    write_json: bool = True
    
    def get_baseline_cache(self, study_dir: Path) -> Path:
        """Get the baseline cache directory."""
        if self.baseline_cache_dir:
            return self.baseline_cache_dir
        return study_dir / "_baseline_cache"
    
    def get_effective_suites(self) -> List[Path]:
        """Get the list of suites to use for ADR."""
        if self.suites:
            return self.suites
        # Default to existing suites
        suites = []
        if RESEARCH_SUITE.exists():
            suites.append(RESEARCH_SUITE)
        adversarial_suite, _ = get_adversarial_suite()
        if adversarial_suite.exists():
            suites.append(adversarial_suite)
        return suites


@dataclass
class ADRResult:
    """Result from ADR report generation."""
    suite_name: str
    baseline_dir: Path
    candidate_dir: Path
    report_md: Optional[Path] = None
    report_json: Optional[Path] = None
    gates_passed: bool = True
    gate_failures: List[str] = field(default_factory=list)
    command_run: str = ""
    returncode: int = 0
    stdout: str = ""
    stderr: str = ""


# ===========================================================================
# Phase B Configuration
# ===========================================================================

@dataclass
class PhaseBConfig:
    """
    Configuration for Phase B: Confidence-Aware Statistical Gating.
    
    Phase B adds bootstrap-based confidence intervals and statistical
    dominance testing to the promotion decision.
    """
    enabled: bool = False
    alpha: float = 0.05  # Significance level for confidence intervals
    n_bootstrap: int = 1000  # Number of bootstrap samples
    seed: int = 42  # Random seed for reproducibility
    kill_threshold: Optional[float] = None  # Max allowed kill rate
    require_strict_dominance: bool = True  # Require candidate to strictly dominate
    baseline_run_dir: Optional[Path] = None  # Path to baseline run (optional)


@dataclass
class PhaseBResult:
    """Result from Phase B gating."""
    passed: bool = True
    decision: Optional[Any] = None  # PhaseBDecision when available
    fail_reasons: List[str] = field(default_factory=list)
    report_json: Optional[Path] = None
    report_md: Optional[Path] = None


# ===========================================================================
# Schemas (inline to avoid import complexity)
# ===========================================================================

@dataclass
class TierBudget:
    """
    Budget constraints for a risk tier (confidence-aware).
    
    A candidate passes the budget if:
    - kill_ucb <= max_kill_ucb (Wilson UCB of kill probability)
    - dd_cvar <= max_dd_cvar (CVaR is already a tail measure)
    - pnl_lcb >= min_pnl_lcb_usd (Normal LCB of mean PnL)
    
    Legacy fields (max_kill_prob, max_drawdown_cvar, min_mean_pnl) are
    kept for backward compatibility but the new fields take precedence.
    """
    tier_name: str
    # New confidence-aware fields
    max_kill_ucb: float
    max_dd_cvar: float
    min_pnl_lcb_usd: float
    # Legacy fields (for backward compatibility)
    max_kill_prob: float = 0.0
    max_drawdown_cvar: float = 0.0
    min_mean_pnl: float = 0.0


@dataclass
class BudgetConfig:
    """Collection of tier budgets with statistical configuration."""
    tiers: Dict[str, TierBudget] = field(default_factory=dict)
    alpha: float = 0.05  # Significance level for confidence bounds


@dataclass
class CandidateConfig:
    """Configuration for a single candidate trial."""
    candidate_id: str
    profile: str
    hedge_band_base: float
    mm_size_eta: float
    vol_ref: float
    daily_loss_limit: float
    init_q_tao: float = 0.0
    hedge_max_step: float = 10.0
    
    def to_env_overlay(self) -> Dict[str, str]:
        """Convert to environment variable overlay."""
        return {
            "PARAPHINA_RISK_PROFILE": self.profile,
            "PARAPHINA_HEDGE_BAND_BASE": str(self.hedge_band_base),
            "PARAPHINA_MM_SIZE_ETA": str(self.mm_size_eta),
            "PARAPHINA_VOL_REF": str(self.vol_ref),
            "PARAPHINA_DAILY_LOSS_LIMIT": str(self.daily_loss_limit),
            "PARAPHINA_INIT_Q_TAO": str(self.init_q_tao),
            "PARAPHINA_HEDGE_MAX_STEP": str(self.hedge_max_step),
        }
    
    def config_hash(self) -> str:
        """Compute deterministic hash of configuration."""
        env = self.to_env_overlay()
        sorted_items = sorted(env.items())
        key_str = "_".join(f"{k}={v}" for k, v in sorted_items)
        return hashlib.sha256(key_str.encode()).hexdigest()[:12]
    
    def write_env_file(self, path: Path) -> None:
        """Write configuration as .env file (candidate.env)."""
        with open(path, "w") as f:
            f.write("# Candidate configuration\n")
            f.write(f"# Generated: {datetime.now(timezone.utc).isoformat()}Z\n")
            f.write(f"# Hash: {self.config_hash()}\n\n")
            for k, v in sorted(self.to_env_overlay().items()):
                f.write(f"export {k}={v}\n")
    
    @classmethod
    def from_env_file(cls, path: Path) -> "CandidateConfig":
        """Parse configuration from .env file."""
        env: Dict[str, str] = {}
        with open(path) as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("export "):
                    line = line[7:]
                if "=" in line:
                    key, val = line.split("=", 1)
                    env[key.strip()] = val.strip()
        
        return cls(
            candidate_id=hashlib.sha256(
                "_".join(f"{k}={v}" for k, v in sorted(env.items())).encode()
            ).hexdigest()[:12],
            profile=env.get("PARAPHINA_RISK_PROFILE", "balanced"),
            hedge_band_base=float(env.get("PARAPHINA_HEDGE_BAND_BASE", "0.05")),
            mm_size_eta=float(env.get("PARAPHINA_MM_SIZE_ETA", "1.0")),
            vol_ref=float(env.get("PARAPHINA_VOL_REF", "0.10")),
            daily_loss_limit=float(env.get("PARAPHINA_DAILY_LOSS_LIMIT", "1000.0")),
            init_q_tao=float(env.get("PARAPHINA_INIT_Q_TAO", "0.0")),
            hedge_max_step=float(env.get("PARAPHINA_HEDGE_MAX_STEP", "10.0")),
        )


@dataclass
class TrialResult:
    """Result from evaluating a single trial."""
    trial_id: str
    candidate_id: str
    config: CandidateConfig
    trial_dir: Path
    
    # Monte Carlo metrics - point estimates
    mc_mean_pnl: float = float("nan")
    mc_pnl_stdev: float = float("nan")
    mc_pnl_cvar: float = float("nan")
    mc_drawdown_cvar: float = float("nan")
    mc_kill_prob_point: float = float("nan")
    mc_kill_prob_ci_upper: float = float("nan")
    mc_total_runs: int = 0
    mc_kill_count: int = 0
    
    # Confidence bounds (computed from point estimates + alpha)
    mc_pnl_lcb: float = float("nan")  # Normal LCB
    mc_kill_ucb: float = float("nan")  # Wilson UCB
    
    # Suite results
    research_passed: bool = False
    adversarial_passed: bool = False
    
    # ADR results (when enabled)
    adr_passed: bool = True  # Default True when ADR not enabled
    adr_results: List[Any] = field(default_factory=list)  # List[ADRResult]
    
    # Scenario Library results (promotion-critical)
    scenario_library_result: Optional[Any] = None  # ScenarioLibraryResult
    
    # Evidence verification
    evidence_verified: bool = False
    evidence_errors: List[str] = field(default_factory=list)
    
    # Execution info
    duration_sec: float = 0.0
    error_message: Optional[str] = None
    seed: int = 42
    
    # Commands run (for reproducibility)
    commands_run: List[str] = field(default_factory=list)
    
    @property
    def is_valid(self) -> bool:
        """
        Check if result is valid for optimization.
        
        A result is valid if:
        - All evidence is verified
        - All required suites passed
        - All required statistics are available (not NaN)
        
        Fail closed: if any required stat is missing, result is invalid.
        """
        return (
            self.evidence_verified
            and self.research_passed
            and self.adversarial_passed
            and self.adr_passed
            and not math.isnan(self.mc_mean_pnl)
            and not math.isnan(self.mc_kill_ucb)
            and not math.isnan(self.mc_pnl_lcb)
            and not math.isnan(self.mc_drawdown_cvar)
        )
    
    def passes_budget(self, budget: TierBudget) -> bool:
        """
        Check if result passes a tier budget using confidence bounds.
        
        Uses confidence-aware gating:
        - kill_ucb <= max_kill_ucb (Wilson UCB)
        - pnl_lcb >= min_pnl_lcb_usd (Normal LCB)
        - dd_cvar <= max_dd_cvar (CVaR is already a tail measure)
        
        Fail closed: if result is not valid, it does not pass.
        """
        if not self.is_valid:
            return False
        return (
            self.mc_kill_ucb <= budget.max_kill_ucb
            and self.mc_drawdown_cvar <= budget.max_dd_cvar
            and self.mc_pnl_lcb >= budget.min_pnl_lcb_usd
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        adr_results_serialized = []
        for adr_result in self.adr_results:
            if hasattr(adr_result, "__dict__"):
                adr_dict = {
                    "suite_name": adr_result.suite_name,
                    "baseline_dir": str(adr_result.baseline_dir),
                    "candidate_dir": str(adr_result.candidate_dir),
                    "report_md": str(adr_result.report_md) if adr_result.report_md else None,
                    "report_json": str(adr_result.report_json) if adr_result.report_json else None,
                    "gates_passed": adr_result.gates_passed,
                    "gate_failures": adr_result.gate_failures,
                    "command_run": adr_result.command_run,
                }
                adr_results_serialized.append(adr_dict)
        
        return {
            "trial_id": self.trial_id,
            "candidate_id": self.candidate_id,
            "config": asdict(self.config),
            "trial_dir": str(self.trial_dir),
            # Point estimates
            "pnl_mean": self.mc_mean_pnl,
            "pnl_stdev": self.mc_pnl_stdev,
            "pnl_cvar": self.mc_pnl_cvar,
            "dd_cvar": self.mc_drawdown_cvar,
            "kill_rate": self.mc_kill_prob_point,
            "kill_k": self.mc_kill_count,
            "kill_n": self.mc_total_runs,
            # Confidence bounds
            "pnl_lcb": self.mc_pnl_lcb,
            "kill_ucb": self.mc_kill_ucb,
            # Legacy fields (for backward compatibility)
            "mc_mean_pnl": self.mc_mean_pnl,
            "mc_pnl_cvar": self.mc_pnl_cvar,
            "mc_drawdown_cvar": self.mc_drawdown_cvar,
            "mc_kill_prob_point": self.mc_kill_prob_point,
            "mc_kill_prob_ci_upper": self.mc_kill_prob_ci_upper,
            "mc_total_runs": self.mc_total_runs,
            # Suite results
            "research_passed": self.research_passed,
            "adversarial_passed": self.adversarial_passed,
            "adr_passed": self.adr_passed,
            "adr_results": adr_results_serialized,
            "scenario_library": self.scenario_library_result.to_dict() if self.scenario_library_result else None,
            "evidence_verified": self.evidence_verified,
            "evidence_errors": self.evidence_errors,
            "is_valid": self.is_valid,
            "duration_sec": self.duration_sec,
            "error_message": self.error_message,
            "seed": self.seed,
            "commands_run": self.commands_run,
        }


# ===========================================================================
# Knob ranges for candidate generation
# ===========================================================================

KNOB_RANGES = {
    "hedge_band_base": [0.02, 0.03, 0.05, 0.075, 0.10, 0.15],
    "mm_size_eta": [0.5, 0.75, 1.0, 1.25, 1.5, 2.0],
    "vol_ref": [0.05, 0.075, 0.10, 0.125, 0.15],
    "daily_loss_limit": [500.0, 750.0, 1000.0, 1500.0, 2000.0, 3000.0],
    "init_q_tao": [-30.0, -15.0, 0.0, 15.0, 30.0],
    "hedge_max_step": [5.0, 10.0, 15.0, 20.0],
}

PROFILES = ["balanced", "conservative", "aggressive"]


# ===========================================================================
# Candidate generation
# ===========================================================================

def generate_candidates(
    n_candidates: int,
    seed: int,
    pareto_set: Optional[List[TrialResult]] = None,
    mutation_frac: float = 0.3,
) -> List[CandidateConfig]:
    """
    Generate candidate configurations using seeded RNG.
    
    Supports optional mutation around the current Pareto set (evolutionary-lite).
    
    Args:
        n_candidates: Number of candidates to generate
        seed: Random seed for determinism
        pareto_set: Optional Pareto set for mutation-based generation
        mutation_frac: Fraction of candidates to generate via mutation (0-1)
    
    Returns:
        List of CandidateConfig objects (deterministic given seed)
    """
    rng = random.Random(seed)
    candidates: List[CandidateConfig] = []
    
    n_mutate = int(n_candidates * mutation_frac) if pareto_set else 0
    n_random = n_candidates - n_mutate
    
    # Generate random candidates
    for i in range(n_random):
        config = CandidateConfig(
            candidate_id="",
            profile=rng.choice(PROFILES),
            hedge_band_base=rng.choice(KNOB_RANGES["hedge_band_base"]),
            mm_size_eta=rng.choice(KNOB_RANGES["mm_size_eta"]),
            vol_ref=rng.choice(KNOB_RANGES["vol_ref"]),
            daily_loss_limit=rng.choice(KNOB_RANGES["daily_loss_limit"]),
            init_q_tao=rng.choice(KNOB_RANGES["init_q_tao"]),
            hedge_max_step=rng.choice(KNOB_RANGES["hedge_max_step"]),
        )
        config = CandidateConfig(
            candidate_id=config.config_hash(),
            profile=config.profile,
            hedge_band_base=config.hedge_band_base,
            mm_size_eta=config.mm_size_eta,
            vol_ref=config.vol_ref,
            daily_loss_limit=config.daily_loss_limit,
            init_q_tao=config.init_q_tao,
            hedge_max_step=config.hedge_max_step,
        )
        candidates.append(config)
    
    # Generate mutated candidates from Pareto set
    if pareto_set and n_mutate > 0:
        for i in range(n_mutate):
            parent = rng.choice(pareto_set)
            
            # Mutate one or two knobs
            n_mutations = rng.randint(1, 2)
            knobs_to_mutate = rng.sample(list(KNOB_RANGES.keys()), n_mutations)
            
            new_config = CandidateConfig(
                candidate_id="",
                profile=parent.config.profile if "profile" not in knobs_to_mutate else rng.choice(PROFILES),
                hedge_band_base=parent.config.hedge_band_base if "hedge_band_base" not in knobs_to_mutate else rng.choice(KNOB_RANGES["hedge_band_base"]),
                mm_size_eta=parent.config.mm_size_eta if "mm_size_eta" not in knobs_to_mutate else rng.choice(KNOB_RANGES["mm_size_eta"]),
                vol_ref=parent.config.vol_ref if "vol_ref" not in knobs_to_mutate else rng.choice(KNOB_RANGES["vol_ref"]),
                daily_loss_limit=parent.config.daily_loss_limit if "daily_loss_limit" not in knobs_to_mutate else rng.choice(KNOB_RANGES["daily_loss_limit"]),
                init_q_tao=parent.config.init_q_tao if "init_q_tao" not in knobs_to_mutate else rng.choice(KNOB_RANGES["init_q_tao"]),
                hedge_max_step=parent.config.hedge_max_step if "hedge_max_step" not in knobs_to_mutate else rng.choice(KNOB_RANGES["hedge_max_step"]),
            )
            new_config = CandidateConfig(
                candidate_id=new_config.config_hash(),
                profile=new_config.profile,
                hedge_band_base=new_config.hedge_band_base,
                mm_size_eta=new_config.mm_size_eta,
                vol_ref=new_config.vol_ref,
                daily_loss_limit=new_config.daily_loss_limit,
                init_q_tao=new_config.init_q_tao,
                hedge_max_step=new_config.hedge_max_step,
            )
            candidates.append(new_config)
    
    return candidates


def generate_smoke_candidates() -> List[CandidateConfig]:
    """Generate fixed smoke test candidates (one per profile)."""
    candidates = []
    for profile in PROFILES:
        config = CandidateConfig(
            candidate_id="",
            profile=profile,
            hedge_band_base=0.05,
            mm_size_eta=1.0,
            vol_ref=0.10,
            daily_loss_limit=1000.0,
            init_q_tao=0.0,
            hedge_max_step=10.0,
        )
        config = CandidateConfig(
            candidate_id=config.config_hash(),
            profile=config.profile,
            hedge_band_base=config.hedge_band_base,
            mm_size_eta=config.mm_size_eta,
            vol_ref=config.vol_ref,
            daily_loss_limit=config.daily_loss_limit,
            init_q_tao=config.init_q_tao,
            hedge_max_step=config.hedge_max_step,
        )
        candidates.append(config)
    return candidates


# ===========================================================================
# Budget loading
# ===========================================================================

def load_budgets_yaml(path: Path) -> BudgetConfig:
    """
    Load budgets from YAML file (simple parser, no PyYAML required).
    
    Supports both new confidence-aware fields and legacy fields:
    - New: max_kill_ucb, max_dd_cvar, min_pnl_lcb_usd, alpha
    - Legacy: max_kill_prob, max_drawdown_cvar, min_mean_pnl
    
    If old fields are present but new fields are missing, the old fields
    are used to populate the new fields for backward compatibility.
    """
    if not path.exists():
        return load_default_budgets()
    
    content = path.read_text()
    tiers: Dict[str, TierBudget] = {}
    current_tier: Optional[str] = None
    tier_data: Dict[str, float] = {}
    global_alpha: float = 0.05
    
    for line in content.split("\n"):
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        
        # Check for global alpha at top level
        if line.startswith("alpha:") and current_tier is None:
            try:
                global_alpha = float(line.split(":", 1)[1].strip())
            except ValueError:
                pass
            continue
        
        if line.endswith(":") and not line.startswith(" "):
            if current_tier and tier_data:
                tiers[current_tier] = _create_tier_budget(current_tier, tier_data)
            current_tier = line[:-1].strip()
            tier_data = {}
        elif ":" in line and current_tier:
            key, val = line.split(":", 1)
            try:
                tier_data[key.strip()] = float(val.strip())
            except ValueError:
                pass
    
    if current_tier and tier_data:
        tiers[current_tier] = _create_tier_budget(current_tier, tier_data)
    
    if not tiers:
        return load_default_budgets()
    
    return BudgetConfig(tiers=tiers, alpha=global_alpha)


def _create_tier_budget(tier_name: str, tier_data: Dict[str, float]) -> TierBudget:
    """
    Create a TierBudget from parsed YAML data.
    
    Handles backward compatibility by mapping old fields to new fields
    if the new fields are not present.
    """
    # New fields (confidence-aware)
    max_kill_ucb = tier_data.get("max_kill_ucb")
    max_dd_cvar = tier_data.get("max_dd_cvar")
    min_pnl_lcb_usd = tier_data.get("min_pnl_lcb_usd")
    
    # Legacy fields
    max_kill_prob = tier_data.get("max_kill_prob", 0.10)
    max_drawdown_cvar = tier_data.get("max_drawdown_cvar", 1000.0)
    min_mean_pnl = tier_data.get("min_mean_pnl", 10.0)
    
    # Map legacy to new if new fields are missing
    if max_kill_ucb is None:
        max_kill_ucb = max_kill_prob
    if max_dd_cvar is None:
        max_dd_cvar = max_drawdown_cvar
    if min_pnl_lcb_usd is None:
        min_pnl_lcb_usd = min_mean_pnl
    
    return TierBudget(
        tier_name=tier_name,
        max_kill_ucb=max_kill_ucb,
        max_dd_cvar=max_dd_cvar,
        min_pnl_lcb_usd=min_pnl_lcb_usd,
        max_kill_prob=max_kill_prob,
        max_drawdown_cvar=max_drawdown_cvar,
        min_mean_pnl=min_mean_pnl,
    )


def load_default_budgets() -> BudgetConfig:
    """Load default budgets with confidence-aware thresholds."""
    return BudgetConfig(
        tiers={
            "conservative": TierBudget(
                tier_name="conservative",
                max_kill_ucb=0.05, max_dd_cvar=500.0, min_pnl_lcb_usd=10.0,
                max_kill_prob=0.05, max_drawdown_cvar=500.0, min_mean_pnl=10.0,
            ),
            "balanced": TierBudget(
                tier_name="balanced",
                max_kill_ucb=0.10, max_dd_cvar=1000.0, min_pnl_lcb_usd=20.0,
                max_kill_prob=0.10, max_drawdown_cvar=1000.0, min_mean_pnl=20.0,
            ),
            "aggressive": TierBudget(
                tier_name="aggressive",
                max_kill_ucb=0.15, max_dd_cvar=2000.0, min_pnl_lcb_usd=30.0,
                max_kill_prob=0.15, max_drawdown_cvar=2000.0, min_mean_pnl=30.0,
            ),
            "research": TierBudget(
                tier_name="research",
                max_kill_ucb=0.20, max_dd_cvar=3000.0, min_pnl_lcb_usd=0.0,
                max_kill_prob=0.20, max_drawdown_cvar=3000.0, min_mean_pnl=0.0,
            ),
        },
        alpha=0.05,
    )


# ===========================================================================
# mc_summary.json parsing
# ===========================================================================

def parse_mc_summary(path: Path) -> Dict[str, Any]:
    """
    Parse mc_summary.json into flat metrics dict.
    
    Extracts all fields needed for confidence-aware gating:
    - mean_pnl, pnl_stdev, pnl_cvar (for normal LCB)
    - kill_count, total_runs (for Wilson UCB)
    - drawdown_cvar (used directly as a tail measure)
    """
    if not path.exists():
        return {}
    
    try:
        with open(path) as f:
            summary = json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}
    
    tail_risk = summary.get("tail_risk", {})
    aggregate = summary.get("aggregate", {})
    
    pnl_var_cvar = tail_risk.get("pnl_var_cvar", {})
    dd_var_cvar = tail_risk.get("max_drawdown_var_cvar", {})
    kill_prob = tail_risk.get("kill_probability", {})
    pnl_agg = aggregate.get("pnl", {})
    
    return {
        # PnL point estimates
        "mean_pnl": pnl_agg.get("mean", float("nan")),
        "pnl_stdev": pnl_agg.get("std_pop", float("nan")),  # Population stdev
        "pnl_cvar": pnl_var_cvar.get("cvar", float("nan")),
        # Drawdown
        "drawdown_cvar": dd_var_cvar.get("cvar", float("nan")),
        # Kill probability
        "kill_prob_point": kill_prob.get("point_estimate", float("nan")),
        "kill_prob_ci_upper": kill_prob.get("ci_upper", float("nan")),
        "kill_count": kill_prob.get("kill_count", 0),
        "total_runs": kill_prob.get("total_runs", 0),
        "kill_rate": aggregate.get("kill_rate", float("nan")),
    }


def compute_confidence_bounds(
    metrics: Dict[str, Any],
    alpha: float = 0.05,
) -> Tuple[float, float]:
    """
    Compute confidence bounds from MC summary metrics.
    
    Returns (pnl_lcb, kill_ucb) using the appropriate statistical methods.
    
    Args:
        metrics: Dictionary from parse_mc_summary
        alpha: Significance level (default 0.05)
        
    Returns:
        Tuple of (pnl_lcb, kill_ucb)
    """
    # Extract required fields
    mean_pnl = metrics.get("mean_pnl", float("nan"))
    pnl_stdev = metrics.get("pnl_stdev", float("nan"))
    total_runs = metrics.get("total_runs", 0)
    kill_count = metrics.get("kill_count", 0)
    
    # Compute PnL LCB using normal interval
    pnl_lcb = float("nan")
    if not math.isnan(mean_pnl) and not math.isnan(pnl_stdev) and total_runs > 0:
        try:
            pnl_lcb = normal_lcb(mean_pnl, pnl_stdev, total_runs, alpha)
        except ValueError:
            pass
    
    # Compute kill UCB using Wilson score
    kill_ucb = float("nan")
    if total_runs > 0:
        try:
            kill_ucb = wilson_ucb(kill_count, total_runs, alpha)
        except ValueError:
            pass
    
    return (pnl_lcb, kill_ucb)


# ===========================================================================
# Evidence verification
# ===========================================================================

def verify_evidence_tree(output_dir: Path, verbose: bool = False) -> Tuple[bool, List[str]]:
    """Verify all evidence packs under output_dir."""
    if not SIM_EVAL_BIN.exists():
        # Try cargo run
        cmd = ["cargo", "run", "-p", "paraphina", "--bin", "sim_eval", "--release", "--",
               "verify-evidence-tree", str(output_dir)]
    else:
        cmd = [str(SIM_EVAL_BIN), "verify-evidence-tree", str(output_dir)]
    
    proc = subprocess.run(cmd, cwd=str(ROOT), capture_output=True, text=True)
    
    if proc.returncode == 0:
        if verbose:
            print(f"    ✓ Evidence verified: {output_dir.name}")
        return True, []
    else:
        errors = []
        if proc.stderr:
            errors.append(proc.stderr.strip())
        if proc.stdout:
            errors.append(proc.stdout.strip())
        if not errors:
            errors.append(f"verify-evidence-tree failed (rc={proc.returncode})")
        return False, errors


# ===========================================================================
# ADR (Adversarial Delta Regression) Functions
# ===========================================================================

def find_evidence_root(path: Path) -> Optional[Path]:
    """
    Find the nearest directory containing evidence packs.
    
    Searches the given path and its subdirectories for evidence packs.
    Returns the path if any evidence pack is found under it.
    
    Args:
        path: Starting path to search from
        
    Returns:
        Path to the evidence root, or None if not found
    """
    current = path.resolve()
    
    # First check if the path itself contains an evidence pack
    if _has_evidence_pack(current):
        return current
    
    # Recursively search subdirectories for evidence packs
    if current.is_dir():
        if _has_any_evidence_pack_recursive(current):
            return current
    
    # Walk up looking for a parent with evidence packs
    for parent in current.parents:
        if _has_evidence_pack(parent):
            return parent
    
    return None


def _has_any_evidence_pack_recursive(directory: Path, max_depth: int = 5) -> bool:
    """
    Recursively check if any subdirectory contains an evidence pack.
    
    Args:
        directory: Directory to search
        max_depth: Maximum depth to search (prevents runaway searches)
        
    Returns:
        True if any evidence pack found under directory
    """
    if max_depth <= 0:
        return False
    
    try:
        for child in directory.iterdir():
            if child.is_dir():
                if _has_evidence_pack(child):
                    return True
                if _has_any_evidence_pack_recursive(child, max_depth - 1):
                    return True
    except (PermissionError, OSError):
        pass
    
    return False


def _has_evidence_pack(directory: Path) -> bool:
    """Check if a directory contains an evidence pack."""
    evidence_pack = directory / "evidence_pack"
    sha256sums = evidence_pack / "SHA256SUMS"
    return sha256sums.exists()


def compute_baseline_cache_path(
    cache_root: Path,
    profile: str,
    suite_path: Path,
) -> Path:
    """
    Compute stable, deterministic baseline cache path.
    
    The baseline cache path is computed as:
        <cache_root>/<profile>/<suite_stem>/
    
    This ensures baselines are reusable across trials for the same
    profile and suite combination.
    
    Args:
        cache_root: Root directory for baseline cache
        profile: Risk profile name (balanced/conservative/aggressive)
        suite_path: Path to the suite YAML file
        
    Returns:
        Deterministic path for the baseline cache
    """
    suite_name = suite_path.stem
    return cache_root / profile / suite_name


def run_baseline_suite(
    suite_path: Path,
    output_dir: Path,
    profile: str,
    env_overlay: Dict[str, str],
    verbose: bool = False,
) -> Tuple[bool, List[str]]:
    """
    Run a suite to generate baseline outputs for ADR.
    
    Args:
        suite_path: Path to suite YAML
        output_dir: Output directory for baseline
        profile: Risk profile to use
        env_overlay: Environment overlay
        verbose: Enable verbose output
        
    Returns:
        (success, errors) tuple
    """
    if verbose:
        print(f"    Baseline suite output: {output_dir}")
    
    return run_suite(suite_path, output_dir, env_overlay, verbose)


def run_adr_report(
    baseline_dir: Path,
    candidate_dir: Path,
    suite_name: str,
    report_dir: Path,
    adr_config: ADRConfig,
    verbose: bool = False,
) -> ADRResult:
    """
    Run sim_eval report comparing baseline vs candidate.
    
    Args:
        baseline_dir: Baseline suite output directory
        candidate_dir: Candidate suite output directory
        suite_name: Name of the suite (for report naming)
        report_dir: Directory to write reports to
        adr_config: ADR configuration
        verbose: Enable verbose output
        
    Returns:
        ADRResult with report paths and gate status
    """
    result = ADRResult(
        suite_name=suite_name,
        baseline_dir=baseline_dir,
        candidate_dir=candidate_dir,
    )
    
    report_dir.mkdir(parents=True, exist_ok=True)
    
    out_md = report_dir / f"{suite_name}.md" if adr_config.write_md else None
    out_json = report_dir / f"{suite_name}.json" if adr_config.write_json else None
    
    # Build command
    if SIM_EVAL_BIN.exists():
        cmd = [str(SIM_EVAL_BIN), "report"]
    else:
        cmd = ["cargo", "run", "-p", "paraphina", "--bin", "sim_eval", "--release", "--", "report"]
    
    cmd.extend(["--baseline", str(baseline_dir)])
    cmd.extend(["--variant", f"candidate={candidate_dir}"])
    
    # Output paths (required by sim_eval report)
    if out_md:
        cmd.extend(["--out-md", str(out_md)])
        result.report_md = out_md
    else:
        # sim_eval requires --out-md, so use a temp path
        temp_md = report_dir / f"{suite_name}_temp.md"
        cmd.extend(["--out-md", str(temp_md)])
    
    if out_json:
        cmd.extend(["--out-json", str(out_json)])
        result.report_json = out_json
    else:
        # sim_eval requires --out-json, so use a temp path
        temp_json = report_dir / f"{suite_name}_temp.json"
        cmd.extend(["--out-json", str(temp_json)])
    
    # Optional gate flags
    if adr_config.gate_max_regression_usd is not None:
        cmd.extend(["--gate-max-regression-usd", str(adr_config.gate_max_regression_usd)])
    
    if adr_config.gate_max_regression_pct is not None:
        cmd.extend(["--gate-max-regression-pct", str(adr_config.gate_max_regression_pct)])
    
    result.command_run = " ".join(cmd)
    
    if verbose:
        print(f"    ADR report: {report_dir / suite_name}.*")
    
    proc = subprocess.run(cmd, cwd=str(ROOT), capture_output=True, text=True)
    
    result.returncode = proc.returncode
    result.stdout = proc.stdout
    result.stderr = proc.stderr
    
    if proc.returncode != 0:
        result.gates_passed = False
        result.gate_failures.append(f"sim_eval report failed (rc={proc.returncode})")
        if proc.stderr:
            result.gate_failures.append(proc.stderr.strip()[:200])
    else:
        # Parse JSON report to check for gate failures
        if out_json and out_json.exists():
            try:
                with open(out_json) as f:
                    report_data = json.load(f)
                result.gates_passed = report_data.get("gates_passed", True)
                result.gate_failures = report_data.get("gate_failures", [])
            except (json.JSONDecodeError, IOError):
                pass
    
    return result


def ensure_baseline_exists(
    suite_path: Path,
    cache_root: Path,
    profile: str,
    env_overlay: Dict[str, str],
    verbose: bool = False,
) -> Tuple[Path, bool, List[str]]:
    """
    Ensure baseline suite output exists, creating if necessary.
    
    Args:
        suite_path: Path to suite YAML
        cache_root: Root directory for baseline cache
        profile: Risk profile
        env_overlay: Environment overlay
        verbose: Enable verbose output
        
    Returns:
        (baseline_dir, newly_created, errors) tuple
    """
    baseline_dir = compute_baseline_cache_path(cache_root, profile, suite_path)
    
    # Check if baseline already exists with evidence pack
    if _has_evidence_pack(baseline_dir):
        if verbose:
            print(f"    Reusing cached baseline: {baseline_dir}")
        return baseline_dir, False, []
    
    # Create baseline
    if verbose:
        print(f"    Creating baseline: {baseline_dir}")
    
    success, errors = run_baseline_suite(
        suite_path, baseline_dir, profile, env_overlay, verbose
    )
    
    if not success:
        return baseline_dir, True, errors
    
    # Verify baseline evidence
    verified, verify_errors = verify_evidence_tree(baseline_dir, verbose)
    if not verified:
        errors.extend(verify_errors)
    
    return baseline_dir, True, errors


def run_adr_for_trial(
    trial_dir: Path,
    profile: str,
    env_overlay: Dict[str, str],
    adr_config: ADRConfig,
    study_dir: Path,
    verbose: bool = False,
) -> Tuple[bool, List[ADRResult]]:
    """
    Run ADR reports for a trial.
    
    For each suite in adr_config.suites:
    1. Ensure baseline exists (or reuse cached)
    2. Run suite for candidate
    3. Generate report comparing baseline vs candidate
    
    Args:
        trial_dir: Trial directory
        profile: Risk profile
        env_overlay: Environment overlay
        adr_config: ADR configuration
        study_dir: Study directory (for baseline cache)
        verbose: Enable verbose output
        
    Returns:
        (all_passed, results) tuple
    """
    if not adr_config.enabled:
        return True, []
    
    results: List[ADRResult] = []
    all_passed = True
    cache_root = adr_config.get_baseline_cache(study_dir)
    
    for suite_path in adr_config.get_effective_suites():
        if not suite_path.exists():
            if verbose:
                print(f"    ⚠ ADR suite not found: {suite_path}")
            continue
        
        suite_name = suite_path.stem
        
        # 1. Ensure baseline exists
        baseline_dir, _, baseline_errors = ensure_baseline_exists(
            suite_path, cache_root, profile, env_overlay, verbose
        )
        
        if baseline_errors:
            result = ADRResult(
                suite_name=suite_name,
                baseline_dir=baseline_dir,
                candidate_dir=trial_dir / "suite" / suite_name,
                gates_passed=False,
                gate_failures=[f"Baseline creation failed: {e}" for e in baseline_errors],
            )
            results.append(result)
            all_passed = False
            continue
        
        # 2. The candidate suite should already have been run during trial evaluation
        candidate_dir = trial_dir / "suite" / suite_name
        
        if not candidate_dir.exists():
            if verbose:
                print(f"    ⚠ Candidate suite output not found: {candidate_dir}")
            # Suite wasn't run for this trial, skip ADR
            continue
        
        if verbose:
            print(f"    Candidate suite output: {candidate_dir}")
        
        # 3. Generate ADR report
        report_dir = trial_dir / "report"
        result = run_adr_report(
            baseline_dir=baseline_dir,
            candidate_dir=candidate_dir,
            suite_name=suite_name,
            report_dir=report_dir,
            adr_config=adr_config,
            verbose=verbose,
        )
        
        results.append(result)
        
        if not result.gates_passed:
            all_passed = False
            if verbose:
                for failure in result.gate_failures:
                    print(f"      ✗ ADR gate failed: {failure}")
        else:
            if verbose:
                print(f"      ✓ ADR gates passed")
        
        # Verify evidence for both baseline and candidate
        evidence_root = find_evidence_root(candidate_dir)
        if evidence_root and verbose:
            print(f"    Verified evidence root: {evidence_root}")
    
    return all_passed, results


# ===========================================================================
# Suite runners
# ===========================================================================

def get_suite_out_dir(suite_path: Path) -> Optional[str]:
    """Extract out_dir from suite YAML."""
    try:
        for line in suite_path.read_text().split("\n"):
            line = line.strip()
            if line.startswith("out_dir:"):
                return line.split(":", 1)[1].strip()
    except Exception:
        pass
    return None


def check_suite_format(suite_path: Path) -> Tuple[bool, str]:
    """
    Check if suite has compatible format.
    
    Returns (compatible, reason) - all suite formats are now supported,
    including inline env_overrides (Phase A-2.1 implementation).
    """
    if not suite_path.exists():
        return False, "file not found"
    
    try:
        content = suite_path.read_text()
        # All formats are now supported:
        # - path-based scenarios: run in-process
        # - inline env_overrides: run as subprocess with merged env
        has_path = "  - path:" in content or "- path:" in content
        has_inline = "env_overrides:" in content and "  - id:" in content
        
        if has_path and has_inline:
            return True, "mixed path and inline scenarios"
        elif has_inline:
            return True, "inline env_overrides (subprocess mode)"
        elif has_path:
            return True, "path-based scenarios"
        else:
            return True, "unknown format, will attempt"
    except Exception as e:
        return False, f"parse error: {e}"


def parse_suite_yaml(suite_path: Path) -> Dict[str, Any]:
    """
    Parse a suite YAML file into a dictionary.
    
    Simple parser that handles:
    - suite_id, out_dir, repeat_runs
    - scenarios with id, path, seed, profile, env_overrides
    """
    if not suite_path.exists():
        return {}
    
    result: Dict[str, Any] = {"scenarios": []}
    current_scenario: Optional[Dict[str, Any]] = None
    in_env_overrides = False
    
    try:
        for line in suite_path.read_text().split("\n"):
            stripped = line.strip()
            
            # Skip comments and empty lines
            if not stripped or stripped.startswith("#"):
                continue
            
            # Top-level keys
            if not line.startswith(" ") and ":" in stripped:
                key, val = stripped.split(":", 1)
                key = key.strip()
                val = val.strip()
                if key == "suite_id":
                    result["suite_id"] = val
                elif key == "out_dir":
                    result["out_dir"] = val
                elif key == "repeat_runs":
                    try:
                        result["repeat_runs"] = int(val)
                    except ValueError:
                        pass
                in_env_overrides = False
                current_scenario = None
                continue
            
            # Scenario list item
            if stripped.startswith("- id:") or stripped.startswith("- path:"):
                if current_scenario:
                    result["scenarios"].append(current_scenario)
                current_scenario = {}
                in_env_overrides = False
                
                if stripped.startswith("- id:"):
                    current_scenario["id"] = stripped.split(":", 1)[1].strip()
                else:
                    current_scenario["path"] = stripped.split(":", 1)[1].strip()
                continue
            
            # Scenario properties
            if current_scenario is not None:
                if stripped.startswith("id:"):
                    current_scenario["id"] = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("path:"):
                    current_scenario["path"] = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("seed:"):
                    try:
                        current_scenario["seed"] = int(stripped.split(":", 1)[1].strip())
                    except ValueError:
                        pass
                elif stripped.startswith("profile:"):
                    current_scenario["profile"] = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("env_overrides:"):
                    in_env_overrides = True
                    current_scenario["env_overrides"] = {}
                elif in_env_overrides and ":" in stripped:
                    key, val = stripped.split(":", 1)
                    key = key.strip()
                    val = val.strip().strip('"').strip("'")
                    if key.startswith("PARAPHINA_"):
                        current_scenario["env_overrides"][key] = val
        
        # Add final scenario
        if current_scenario:
            result["scenarios"].append(current_scenario)
    except Exception:
        pass
    
    return result


def merge_env_overlays(
    candidate_env: Dict[str, str],
    suite_scenario_env: Dict[str, str],
) -> Dict[str, str]:
    """
    Merge candidate env overlay with suite scenario env_overrides.
    
    Priority (highest to lowest):
    1. suite scenario env_overrides (stress test params)
    2. candidate config env overlay (tuning params)
    3. existing environment (os.environ)
    
    This allows adversarial suites to override candidate settings.
    """
    merged = candidate_env.copy()
    merged.update(suite_scenario_env)
    return merged


def run_suite(
    suite_path: Path,
    output_dir: Path,
    env_overlay: Dict[str, str],
    verbose: bool = False,
) -> Tuple[bool, List[str]]:
    """
    Run a suite using sim_eval suite command with --output-dir.
    
    ALWAYS uses --output-dir for institutional-grade output isolation.
    Merges candidate env_overlay with suite scenario env_overrides.
    
    Returns (passed, errors).
    """
    if not suite_path.exists():
        return False, [f"Suite file not found: {suite_path}"]
    
    # Check suite format (for logging purposes only, all formats supported)
    compatible, reason = check_suite_format(suite_path)
    if not compatible:
        if verbose:
            print(f"    Suite {suite_path.name}: {reason}")
        return False, [f"Suite check failed: {reason}"]
    
    # Parse suite to get env_overrides from scenarios
    suite_data = parse_suite_yaml(suite_path)
    
    # Build merged env with all scenario env_overrides
    # (sim_eval will handle per-scenario overrides, but we provide base env)
    merged_env = os.environ.copy()
    merged_env.update(env_overlay)
    
    # If suite has inline scenarios with env_overrides, those override candidate env
    for scenario in suite_data.get("scenarios", []):
        scenario_env = scenario.get("env_overrides", {})
        if scenario_env:
            merged_env = merge_env_overlays(merged_env, scenario_env)
    
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ALWAYS use --output-dir for institutional-grade output isolation
    if SIM_EVAL_BIN.exists():
        cmd = [str(SIM_EVAL_BIN), "suite", str(suite_path),
               "--output-dir", str(output_dir), "--verbose"]
    else:
        cmd = ["cargo", "run", "-p", "paraphina", "--bin", "sim_eval", "--release", "--",
               "suite", str(suite_path), "--output-dir", str(output_dir), "--verbose"]
    
    if verbose:
        print(f"    Running suite: {suite_path.name} -> {output_dir}")
        print(f"      Format: {reason}")
    
    proc = subprocess.run(cmd, env=merged_env, cwd=str(ROOT), capture_output=True, text=True)
    
    if proc.returncode == 0:
        return True, []
    else:
        errors = []
        if proc.stderr:
            errors.append(proc.stderr.strip()[:200])
        if proc.stdout:
            # Include last few lines of stdout which may have failure info
            stdout_lines = proc.stdout.strip().split('\n')
            if stdout_lines:
                errors.append("; ".join(stdout_lines[-3:])[:200])
        return False, errors


# ===========================================================================
# Monte Carlo runner
# ===========================================================================

def run_monte_carlo(
    output_dir: Path,
    env_overlay: Dict[str, str],
    mc_runs: int,
    mc_ticks: int,
    seed: int,
    verbose: bool = False,
) -> Tuple[bool, Dict[str, Any], List[str]]:
    """Run monte_carlo evaluation."""
    if MONTE_CARLO_BIN.exists():
        cmd = [
            str(MONTE_CARLO_BIN),
            "--runs", str(mc_runs),
            "--ticks", str(mc_ticks),
            "--seed", str(seed),
            "--jitter-ms", "100",
            "--output-dir", str(output_dir),
            "--quiet",
        ]
    else:
        cmd = [
            "cargo", "run", "-p", "paraphina", "--bin", "monte_carlo", "--release", "--",
            "--runs", str(mc_runs),
            "--ticks", str(mc_ticks),
            "--seed", str(seed),
            "--jitter-ms", "100",
            "--output-dir", str(output_dir),
            "--quiet",
        ]
    
    env = os.environ.copy()
    env.update(env_overlay)
    
    if verbose:
        print(f"    Running monte_carlo: {mc_runs} runs, seed={seed}")
    
    proc = subprocess.run(cmd, env=env, cwd=str(ROOT), capture_output=True, text=True)
    
    if proc.returncode != 0:
        errors = []
        if proc.stderr:
            errors.append(proc.stderr.strip()[:200])
        return False, {}, errors
    
    # Parse mc_summary.json
    summary_path = output_dir / "mc_summary.json"
    metrics = parse_mc_summary(summary_path)
    
    if not metrics:
        return False, {}, ["Failed to parse mc_summary.json"]
    
    return True, metrics, []


# ===========================================================================
# Trial evaluation
# ===========================================================================

def evaluate_trial(
    config: CandidateConfig,
    study_name: str,
    trial_id: str,
    study_dir: Path,
    mc_runs: int,
    mc_ticks: int,
    seed: int,
    verbose: bool = True,
    adr_config: Optional[ADRConfig] = None,
    alpha: float = 0.05,
    scenario_library_config: Optional[ScenarioLibraryConfig] = None,
) -> TrialResult:
    """
    Evaluate a single candidate trial.
    
    Contract:
    1. Create isolated trial directory: runs/phaseA/<study>/<trial_id>/
    2. Write candidate.env
    3. Run monte_carlo to <trial>/mc/
    4. Verify evidence pack (hard fail if verification fails)
    5. Run research suite with --output-dir <trial>/suite/research_v1/
    6. Run adversarial suite with --output-dir <trial>/suite/adversarial_regression_v1/
    7. Verify evidence tree for entire trial (hard fail if verification fails)
    8. Parse metrics from mc_summary.json
    
    CRITICAL: Adversarial suites are NEVER skipped. Missing suite = trial failure.
    """
    t0 = time.time()
    
    # Create trial directory
    trial_dir = study_dir / trial_id
    trial_dir.mkdir(parents=True, exist_ok=True)
    
    if verbose:
        print(f"\n  [{trial_id}] Candidate {config.candidate_id}")
    
    # Write candidate.env
    config_file = trial_dir / "candidate.env"
    config.write_env_file(config_file)
    
    env_overlay = config.to_env_overlay()
    evidence_errors: List[str] = []
    
    # Defaults
    mc_mean_pnl = float("nan")
    mc_pnl_stdev = float("nan")
    mc_pnl_cvar = float("nan")
    mc_drawdown_cvar = float("nan")
    mc_kill_prob_point = float("nan")
    mc_kill_prob_ci_upper = float("nan")
    mc_total_runs = 0
    mc_kill_count = 0
    mc_pnl_lcb = float("nan")
    mc_kill_ucb = float("nan")
    evidence_verified = False
    research_passed = False  # Default to False - must explicitly pass
    adversarial_passed = False  # Default to False - must explicitly pass
    error_message: Optional[str] = None
    
    # Step 1: Run Monte Carlo
    mc_dir = trial_dir / "mc"
    mc_dir.mkdir(exist_ok=True)
    
    success, metrics, errors = run_monte_carlo(
        mc_dir, env_overlay, mc_runs, mc_ticks, seed, verbose
    )
    
    if success:
        mc_mean_pnl = metrics.get("mean_pnl", float("nan"))
        mc_pnl_stdev = metrics.get("pnl_stdev", float("nan"))
        mc_pnl_cvar = metrics.get("pnl_cvar", float("nan"))
        mc_drawdown_cvar = metrics.get("drawdown_cvar", float("nan"))
        mc_kill_prob_point = metrics.get("kill_prob_point", float("nan"))
        mc_kill_prob_ci_upper = metrics.get("kill_prob_ci_upper", float("nan"))
        mc_total_runs = metrics.get("total_runs", 0)
        mc_kill_count = metrics.get("kill_count", 0)
        
        # Compute confidence bounds
        mc_pnl_lcb, mc_kill_ucb = compute_confidence_bounds(metrics, alpha)
        
        # Verify MC evidence pack
        verified, errs = verify_evidence_tree(mc_dir, verbose)
        if not verified:
            evidence_errors.extend(errs)
            if verbose:
                print(f"    ✗ MC evidence verification FAILED")
    else:
        evidence_errors.extend(errors)
        error_message = f"Monte Carlo failed: {'; '.join(errors)}"
    
    # Step 2: Run research suite with --output-dir (REQUIRED)
    # Suite output goes to: <trial>/suite/research_v1/
    research_suite_dir = trial_dir / "suite" / "research_v1"
    
    if RESEARCH_SUITE.exists():
        passed, errors = run_suite(RESEARCH_SUITE, research_suite_dir, env_overlay, verbose)
        research_passed = passed
        if not passed:
            evidence_errors.extend(errors)
            if verbose:
                print(f"    ✗ Research suite FAILED")
        else:
            # Verify research suite evidence
            verified, errs = verify_evidence_tree(research_suite_dir, verbose)
            if not verified:
                evidence_errors.extend(errs)
                research_passed = False
                if verbose:
                    print(f"    ✗ Research suite evidence verification FAILED")
    else:
        # Missing suite file is a configuration error, not skippable
        if verbose:
            print(f"    ✗ Research suite file not found: {RESEARCH_SUITE}")
        evidence_errors.append(f"Research suite file not found: {RESEARCH_SUITE}")
    
    # Step 3: Run adversarial regression suite with --output-dir (REQUIRED - NEVER SKIP)
    # Prefer v2 (path-based) if available, fallback to v1
    adversarial_suite, adv_version = get_adversarial_suite()
    suite_name = adversarial_suite.stem  # adversarial_regression_v1 or v2
    adversarial_suite_dir = trial_dir / "suite" / suite_name
    
    if adversarial_suite.exists():
        passed, errors = run_suite(adversarial_suite, adversarial_suite_dir, env_overlay, verbose)
        adversarial_passed = passed
        if not passed:
            evidence_errors.extend(errors)
            if verbose:
                print(f"    ✗ Adversarial suite FAILED ({adv_version})")
        else:
            # Verify adversarial suite evidence
            verified, errs = verify_evidence_tree(adversarial_suite_dir, verbose)
            if not verified:
                evidence_errors.extend(errs)
                adversarial_passed = False
                if verbose:
                    print(f"    ✗ Adversarial suite evidence verification FAILED")
    else:
        # Missing adversarial suite is a HARD FAILURE - never skip
        if verbose:
            print(f"    ✗ CRITICAL: Adversarial suite file not found: {adversarial_suite}")
        evidence_errors.append(f"CRITICAL: Adversarial suite file not found: {adversarial_suite}")
    
    # Step 4: Final evidence verification for entire trial directory
    # This catches any missed evidence packs and enforces provenance
    if verbose:
        print(f"    Verifying trial evidence tree: {trial_dir}")
    
    final_verified, final_errs = verify_evidence_tree(trial_dir, verbose)
    if not final_verified:
        evidence_errors.extend(final_errs)
        if verbose:
            print(f"    ✗ Trial evidence tree verification FAILED")
    
    # Step 5: ADR (Adversarial Delta Regression) if enabled
    adr_passed = True
    adr_results: List[ADRResult] = []
    commands_run: List[str] = []
    
    if adr_config and adr_config.enabled:
        if verbose:
            print(f"    Running ADR gating...")
        
        adr_passed, adr_results = run_adr_for_trial(
            trial_dir=trial_dir,
            profile=config.profile,
            env_overlay=env_overlay,
            adr_config=adr_config,
            study_dir=study_dir,
            verbose=verbose,
        )
        
        # Collect commands run for reproducibility
        for adr_result in adr_results:
            if adr_result.command_run:
                commands_run.append(adr_result.command_run)
        
        if not adr_passed:
            if verbose:
                print(f"    ✗ ADR gating FAILED")
        else:
            if verbose:
                print(f"    ✓ ADR gating passed")
    
    # Step 6: Scenario Library suite (promotion-critical, default enabled)
    scenario_library_result = ScenarioLibraryResult()
    
    if scenario_library_config and scenario_library_config.enabled:
        scenario_library_suite = scenario_library_config.get_suite_path()
        scenario_library_suite_name = scenario_library_suite.stem
        scenario_library_suite_dir = trial_dir / "suite" / scenario_library_suite_name
        
        if verbose:
            print(f"    Running scenario library suite: {scenario_library_suite.name}")
        
        scenario_library_result.suite_path = str(scenario_library_suite)
        scenario_library_result.output_dir = str(scenario_library_suite_dir)
        
        if not scenario_library_suite.exists():
            scenario_library_result.ran = False
            scenario_library_result.passed = False
            scenario_library_result.errors.append(f"Scenario library suite not found: {scenario_library_suite}")
            evidence_errors.append(f"Scenario library suite not found: {scenario_library_suite}")
            if verbose:
                print(f"    ✗ Scenario library suite file not found")
        else:
            passed, errors = run_suite(scenario_library_suite, scenario_library_suite_dir, env_overlay, verbose)
            scenario_library_result.ran = True
            scenario_library_result.passed = passed
            
            if not passed:
                scenario_library_result.errors.extend(errors)
                evidence_errors.extend(errors)
                if verbose:
                    print(f"    ✗ Scenario library suite FAILED")
            else:
                # Verify scenario library suite evidence
                verified, errs = verify_evidence_tree(scenario_library_suite_dir, verbose)
                scenario_library_result.evidence_verified = verified
                if not verified:
                    scenario_library_result.errors.extend(errs)
                    evidence_errors.extend(errs)
                    if verbose:
                        print(f"    ✗ Scenario library suite evidence verification FAILED")
                else:
                    if verbose:
                        print(f"    ✓ Scenario library suite passed")
    elif scenario_library_config and not scenario_library_config.enabled:
        scenario_library_result.skipped = True
        scenario_library_result.skip_reason = "Scenario library skipped (--skip-scenario-library)"
        if verbose:
            print(f"    ⚠ WARNING: Scenario library SKIPPED (institutional exception)")
    
    # Evidence is verified only if ALL verification steps passed
    scenario_library_passed = (
        scenario_library_result.skipped or  # Skipped is allowed (but recorded)
        (scenario_library_result.ran and scenario_library_result.passed and scenario_library_result.evidence_verified)
    ) if scenario_library_config and scenario_library_config.enabled else True
    
    evidence_verified = (
        final_verified
        and research_passed
        and adversarial_passed
        and scenario_library_passed
        and not error_message
    )
    
    duration_sec = time.time() - t0
    
    result = TrialResult(
        trial_id=trial_id,
        candidate_id=config.candidate_id,
        config=config,
        trial_dir=trial_dir,
        mc_mean_pnl=mc_mean_pnl,
        mc_pnl_stdev=mc_pnl_stdev,
        mc_pnl_cvar=mc_pnl_cvar,
        mc_drawdown_cvar=mc_drawdown_cvar,
        mc_kill_prob_point=mc_kill_prob_point,
        mc_kill_prob_ci_upper=mc_kill_prob_ci_upper,
        mc_total_runs=mc_total_runs,
        mc_kill_count=mc_kill_count,
        mc_pnl_lcb=mc_pnl_lcb,
        mc_kill_ucb=mc_kill_ucb,
        research_passed=research_passed,
        adversarial_passed=adversarial_passed,
        adr_passed=adr_passed,
        adr_results=adr_results,
        scenario_library_result=scenario_library_result,
        evidence_verified=evidence_verified,
        evidence_errors=evidence_errors,
        duration_sec=duration_sec,
        error_message=error_message,
        seed=seed,
        commands_run=commands_run,
    )
    
    if verbose:
        status = "✓ VALID" if result.is_valid else "✗ INVALID"
        print(f"    {status} | pnl_mean={mc_mean_pnl:.2f} pnl_lcb={mc_pnl_lcb:.2f}, kill_ucb={mc_kill_ucb:.3f}, "
              f"dd_cvar={mc_drawdown_cvar:.2f} | {duration_sec:.1f}s")
    
    return result


# ===========================================================================
# Pareto frontier
# ===========================================================================

def compute_pareto_frontier(results: List[TrialResult]) -> List[TrialResult]:
    """Compute Pareto frontier from trial results."""
    valid = [r for r in results if r.is_valid]
    
    if not valid:
        return []
    
    frontier: List[TrialResult] = []
    
    for i, candidate in enumerate(valid):
        dominated = False
        for j, other in enumerate(valid):
            if i != j and _dominates(other, candidate):
                dominated = True
                break
        if not dominated:
            frontier.append(candidate)
    
    # Sort deterministically by candidate_id
    frontier.sort(key=lambda r: r.candidate_id)
    return frontier


def _dominates(a: TrialResult, b: TrialResult) -> bool:
    """Check if a dominates b (Pareto dominance)."""
    # Objectives: maximize pnl, minimize kill_prob, minimize drawdown_cvar
    at_least_as_good = (
        a.mc_mean_pnl >= b.mc_mean_pnl
        and a.mc_kill_prob_ci_upper <= b.mc_kill_prob_ci_upper
        and a.mc_drawdown_cvar <= b.mc_drawdown_cvar
    )
    
    if not at_least_as_good:
        return False
    
    strictly_better = (
        a.mc_mean_pnl > b.mc_mean_pnl
        or a.mc_kill_prob_ci_upper < b.mc_kill_prob_ci_upper
        or a.mc_drawdown_cvar < b.mc_drawdown_cvar
    )
    
    return strictly_better


# ===========================================================================
# Winner selection
# ===========================================================================

def select_winner(results: List[TrialResult], budget: TierBudget) -> Optional[TrialResult]:
    """
    Select winner for a budget tier with deterministic tie-breaking.
    
    Selection rule:
    1. Filter by budget
    2. Best mean_pnl (highest)
    3. Tie-break: lowest drawdown_cvar
    4. Tie-break: lowest kill_prob_ci_upper
    5. Tie-break: candidate_id (alphabetical)
    """
    qualifying = [r for r in results if r.passes_budget(budget)]
    
    if not qualifying:
        return None
    
    qualifying.sort(key=lambda r: (
        -r.mc_mean_pnl,
        r.mc_drawdown_cvar,
        r.mc_kill_prob_ci_upper,
        r.candidate_id,
    ))
    
    return qualifying[0]


# ===========================================================================
# Output writing
# ===========================================================================

def write_trials_jsonl(results: List[TrialResult], path: Path) -> None:
    """Write trials to JSONL (one JSON per line)."""
    with open(path, "w") as f:
        for r in sorted(results, key=lambda x: x.trial_id):
            f.write(json.dumps(r.to_dict(), sort_keys=True) + "\n")


def write_pareto_json(pareto: List[TrialResult], path: Path) -> None:
    """Write Pareto frontier to JSON."""
    data = {
        "schema_version": 1,
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "pareto_size": len(pareto),
        "candidates": [r.to_dict() for r in pareto],
    }
    with open(path, "w") as f:
        json.dump(data, f, indent=2, sort_keys=True)


def write_pareto_csv(pareto: List[TrialResult], path: Path) -> None:
    """Write Pareto frontier to CSV with confidence bounds."""
    import csv
    
    if not pareto:
        path.write_text("# Empty Pareto frontier\n")
        return
    
    fieldnames = [
        "pareto_rank", "candidate_id", "trial_id", "profile",
        # Point estimates
        "pnl_mean", "pnl_stdev", "kill_rate", "dd_cvar",
        # Confidence bounds
        "pnl_lcb", "kill_ucb",
        # Kill statistics
        "kill_k", "kill_n",
        # Status
        "is_valid", "evidence_verified",
    ]
    
    with open(path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for i, r in enumerate(pareto):
            writer.writerow({
                "pareto_rank": i + 1,
                "candidate_id": r.candidate_id,
                "trial_id": r.trial_id,
                "profile": r.config.profile,
                # Point estimates
                "pnl_mean": f"{r.mc_mean_pnl:.4f}",
                "pnl_stdev": f"{r.mc_pnl_stdev:.4f}",
                "kill_rate": f"{r.mc_kill_prob_point:.4f}",
                "dd_cvar": f"{r.mc_drawdown_cvar:.4f}",
                # Confidence bounds
                "pnl_lcb": f"{r.mc_pnl_lcb:.4f}",
                "kill_ucb": f"{r.mc_kill_ucb:.4f}",
                # Kill statistics
                "kill_k": r.mc_kill_count,
                "kill_n": r.mc_total_runs,
                # Status
                "is_valid": r.is_valid,
                "evidence_verified": r.evidence_verified,
            })


# ===========================================================================
# Promotion
# ===========================================================================

def promote_winner(
    winner: TrialResult,
    tier_name: str,
    study_name: str,
    output_dir: Optional[Path] = None,
    verbose: bool = True,
    alpha: float = 0.05,
    seed_schedule: Optional[List[int]] = None,
) -> Tuple[Path, Path]:
    """Promote a winning candidate configuration with full provenance."""
    if output_dir is None:
        output_dir = PROMOTED_DIR / tier_name
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    config_name = f"phaseA_{study_name}_{timestamp}.env"
    record_name = f"PROMOTION_RECORD.json"
    
    config_path = output_dir / config_name
    record_path = output_dir / record_name
    
    if verbose:
        print(f"  Promoting {winner.candidate_id} -> {config_path}")
    
    # Write config file
    winner.config.write_env_file(config_path)
    
    # Add promotion header
    with open(config_path, "r") as f:
        content = f.read()
    with open(config_path, "w") as f:
        f.write(f"# Promoted configuration for {tier_name} tier\n")
        f.write(f"# Study: {study_name}\n")
        f.write(f"# Candidate: {winner.candidate_id}\n")
        f.write(f"# Metrics: pnl={winner.mc_mean_pnl:.4f}, kill_ci={winner.mc_kill_prob_ci_upper:.4f}\n")
        f.write(content)
    
    # Compute config hash
    with open(config_path, "rb") as f:
        config_hash = hashlib.sha256(f.read()).hexdigest()
    
    # Get git commit
    try:
        git_result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            cwd=str(ROOT), capture_output=True, text=True
        )
        git_commit = git_result.stdout.strip() if git_result.returncode == 0 else "unknown"
    except Exception:
        git_commit = "unknown"
    
    # Build commands list
    commands = [
        f"monte_carlo --runs {winner.mc_total_runs} --seed {winner.seed}",
        f"sim_eval suite {RESEARCH_SUITE}",
        f"sim_eval suite {get_adversarial_suite()[0]}",
    ]
    # Add any ADR commands
    if winner.commands_run:
        commands.extend(winner.commands_run)
    
    # Build promotion record with confidence-aware statistics
    record = {
        "schema_version": 3,  # v3 adds statistics section with confidence bounds
        "promoted_at": datetime.now(timezone.utc).isoformat(),
        "study_name": study_name,
        "tier": tier_name,
        "git_commit": git_commit,
        "candidate_id": winner.candidate_id,
        "candidate_hash": config_hash,
        "trial_id": winner.trial_id,
        "trial_dir": str(winner.trial_dir),
        "config": asdict(winner.config),
        "env_overlay": winner.config.to_env_overlay(),
        "commands_run": commands,
        "seeds": seed_schedule if seed_schedule else [winner.seed],
        "evidence_verified": winner.evidence_verified,
        # Point estimates
        "metrics": {
            "pnl_mean": winner.mc_mean_pnl,
            "pnl_stdev": winner.mc_pnl_stdev,
            "pnl_cvar": winner.mc_pnl_cvar,
            "dd_cvar": winner.mc_drawdown_cvar,
            "kill_rate": winner.mc_kill_prob_point,
            "kill_k": winner.mc_kill_count,
            "kill_n": winner.mc_total_runs,
            # Confidence bounds
            "pnl_lcb": winner.mc_pnl_lcb,
            "kill_ucb": winner.mc_kill_ucb,
            # Legacy fields
            "mean_pnl": winner.mc_mean_pnl,
            "drawdown_cvar": winner.mc_drawdown_cvar,
            "kill_prob_point": winner.mc_kill_prob_point,
            "kill_prob_ci_upper": winner.mc_kill_prob_ci_upper,
            "total_runs": winner.mc_total_runs,
        },
        # Statistics metadata (new in v3)
        "statistics": get_statistics_metadata(alpha),
        "suites_passed": {
            "research": winner.research_passed,
            "adversarial": winner.adversarial_passed,
            "adr": winner.adr_passed,
        },
        # Scenario Library v1 (promotion-critical)
        "scenario_library": winner.scenario_library_result.to_dict() if winner.scenario_library_result else {
            "ran": False,
            "skipped": True,
            "skip_reason": "Not configured",
            "suite_path": None,
            "output_dir": None,
            "passed": False,
            "errors": [],
            "evidence_verified": False,
        },
    }
    
    # Add ADR results if present
    if winner.adr_results:
        adr_info = []
        for adr_result in winner.adr_results:
            if hasattr(adr_result, "__dict__"):
                adr_info.append({
                    "suite_name": adr_result.suite_name,
                    "gates_passed": adr_result.gates_passed,
                    "report_md": str(adr_result.report_md) if adr_result.report_md else None,
                    "report_json": str(adr_result.report_json) if adr_result.report_json else None,
                })
        record["adr_results"] = adr_info
    
    with open(record_path, "w") as f:
        json.dump(record, f, indent=2, sort_keys=True)
    
    return config_path, record_path


# ===========================================================================
# Phase B Statistical Gate
# ===========================================================================

def run_phase_b_gate(
    winner: "TrialResult",
    study_dir: Path,
    phase_b_config: PhaseBConfig,
    verbose: bool = True,
) -> PhaseBResult:
    """
    Run Phase B confidence-aware statistical gating for a winner candidate.
    
    Phase B uses bootstrap-based confidence intervals to ensure statistically
    rigorous promotion decisions.
    
    Args:
        winner: The winning TrialResult from Phase A
        study_dir: Study directory for output
        phase_b_config: Phase B configuration
        verbose: Print progress
        
    Returns:
        PhaseBResult with pass/fail status and reasons
    """
    if not phase_b_config.enabled:
        return PhaseBResult(passed=True)
    
    if not PHASE_B_AVAILABLE:
        if verbose:
            print("    ⚠ Phase B not available (import failed) - skipping")
        return PhaseBResult(passed=True, fail_reasons=["Phase B module not available"])
    
    if verbose:
        print(f"    Running Phase B confidence gate...")
    
    try:
        # Create gate with configuration
        gate = PhaseBGate(
            alpha=phase_b_config.alpha,
            n_bootstrap=phase_b_config.n_bootstrap,
            seed=phase_b_config.seed,
            kill_threshold=phase_b_config.kill_threshold,
            require_strict_dominance=phase_b_config.require_strict_dominance,
        )
        
        # Load candidate data from MC directory
        mc_dir = winner.trial_dir / "mc"
        if not mc_dir.exists():
            return PhaseBResult(
                passed=False,
                fail_reasons=[f"MC directory not found: {mc_dir}"]
            )
        
        # Load run data
        from batch_runs.phase_b.gate import load_run_data
        candidate_pnl, candidate_kills = load_run_data(mc_dir)
        
        # Load baseline if provided
        baseline_pnl = None
        baseline_kills = None
        baseline_path = None
        
        if phase_b_config.baseline_run_dir is not None:
            baseline_path = phase_b_config.baseline_run_dir
            if baseline_path.exists():
                try:
                    baseline_pnl, baseline_kills = load_run_data(baseline_path)
                except Exception as e:
                    if verbose:
                        print(f"    ⚠ Could not load baseline: {e}")
        
        # Run the gate
        decision = gate.evaluate(
            candidate_pnl=candidate_pnl,
            candidate_kills=candidate_kills,
            baseline_pnl=baseline_pnl,
            baseline_kills=baseline_kills,
            candidate_path=mc_dir,
            baseline_path=baseline_path,
        )
        
        # Write reports
        report_dir = winner.trial_dir / "phase_b_gate"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        json_path = report_dir / "confidence_report.json"
        with open(json_path, "w") as f:
            json.dump(decision.to_dict(), f, indent=2, default=str)
        
        md_path = report_dir / "confidence_report.md"
        with open(md_path, "w") as f:
            f.write(decision.to_markdown())
        
        if verbose:
            print(f"    ✓ Phase B report: {json_path}")
        
        # Check result
        if decision.outcome == PhaseBOutcome.PROMOTE:
            if verbose:
                print(f"    ✓ Phase B gate PASSED")
            return PhaseBResult(
                passed=True,
                decision=decision,
                report_json=json_path,
                report_md=md_path,
            )
        else:
            if verbose:
                print(f"    ✗ Phase B gate FAILED")
                for reason in decision.fail_reasons:
                    print(f"      - {reason}")
            return PhaseBResult(
                passed=False,
                decision=decision,
                fail_reasons=decision.fail_reasons,
                report_json=json_path,
                report_md=md_path,
            )
    
    except Exception as e:
        if verbose:
            print(f"    ✗ Phase B gate error: {e}")
        return PhaseBResult(
            passed=False,
            fail_reasons=[f"Phase B error: {e}"]
        )


# ===========================================================================
# Main pipeline
# ===========================================================================

def run_pipeline(
    study_name: str,
    study_dir: Path,
    trials: int,
    mc_runs: int,
    mc_ticks: int,
    seed: int,
    budgets: BudgetConfig,
    smoke: bool = False,
    verbose: bool = True,
    adr_config: Optional[ADRConfig] = None,
    phase_b_config: Optional[PhaseBConfig] = None,
    scenario_library_config: Optional[ScenarioLibraryConfig] = None,
) -> Tuple[List[TrialResult], List[TrialResult], Dict[str, Optional[Tuple[Path, Path]]]]:
    """
    Run the complete promotion pipeline.
    
    Returns:
        (all_results, pareto_frontier, promotions_by_tier)
    """
    print("=" * 70)
    print(f"Phase A-2: Promotion Pipeline")
    print("=" * 70)
    print(f"  Study: {study_name}")
    print(f"  Directory: {study_dir}")
    print(f"  Trials: {trials}")
    print(f"  MC runs: {mc_runs}, ticks: {mc_ticks}")
    print(f"  Seed: {seed}")
    print(f"  Smoke mode: {smoke}")
    
    # ADR info
    if adr_config and adr_config.enabled:
        print(f"  ADR enabled: True")
        print(f"  ADR baseline cache: {adr_config.get_baseline_cache(study_dir)}")
        if adr_config.gate_max_regression_usd is not None:
            print(f"  ADR max regression USD: {adr_config.gate_max_regression_usd}")
        if adr_config.gate_max_regression_pct is not None:
            print(f"  ADR max regression %: {adr_config.gate_max_regression_pct}")
    else:
        print(f"  ADR enabled: False")
    
    # Phase B info
    if phase_b_config and phase_b_config.enabled:
        print(f"  Phase B enabled: True")
        print(f"  Phase B alpha: {phase_b_config.alpha}")
        print(f"  Phase B n_bootstrap: {phase_b_config.n_bootstrap}")
        if phase_b_config.kill_threshold is not None:
            print(f"  Phase B kill threshold: {phase_b_config.kill_threshold}")
        if phase_b_config.baseline_run_dir is not None:
            print(f"  Phase B baseline: {phase_b_config.baseline_run_dir}")
    else:
        print(f"  Phase B enabled: False")
    
    # Scenario Library info
    if scenario_library_config and scenario_library_config.enabled:
        suite_path = scenario_library_config.get_suite_path()
        print(f"  Scenario Library enabled: True")
        print(f"  Scenario Library suite: {suite_path}")
        if scenario_library_config.smoke_mode:
            print(f"  Scenario Library mode: smoke (5 scenarios)")
        else:
            print(f"  Scenario Library mode: full (10 scenarios)")
    elif scenario_library_config and not scenario_library_config.enabled:
        print(f"  Scenario Library enabled: False (SKIPPED - institutional exception)")
        print(f"  ⚠ WARNING: Scenario library is being skipped. Promotion artifacts may not meet full OOS evaluation requirements.")
    else:
        print(f"  Scenario Library enabled: True (default)")
        # Create default config if not provided
        scenario_library_config = ScenarioLibraryConfig(enabled=True, smoke_mode=smoke)
        print(f"  Scenario Library suite: {scenario_library_config.get_suite_path()}")
    print()
    
    # [Pre-flight] Enforce scenario library integrity at pipeline start
    if scenario_library_config and scenario_library_config.enabled:
        print("[0/6] Verifying scenario library integrity...")
        integrity_ok, integrity_errors = check_scenario_library_integrity(verbose=verbose)
        if not integrity_ok:
            print("=" * 70)
            print("FATAL: Scenario library integrity check failed!")
            print("=" * 70)
            print("The scenario library manifest does not match the files on disk.")
            print("This may indicate:")
            print("  - Uncommitted modifications to scenario files")
            print("  - Manifest was not regenerated after file changes")
            print()
            print("To fix:")
            print("  python3 -m batch_runs.phase_a.scenario_library_v1 generate --seed <seed>")
            print("  python3 -m batch_runs.phase_a.scenario_library_v1 check")
            print("  git add scenarios/v1/scenario_library_v1/")
            print("  git commit -m 'Regenerate scenario library'")
            print()
            raise RuntimeError(f"Scenario library integrity check failed: {integrity_errors}")
        
        # Validate suite file
        suite_path = scenario_library_config.get_suite_path()
        suite_ok, suite_errors = validate_scenario_library_suite(suite_path, verbose=verbose)
        if not suite_ok:
            raise RuntimeError(f"Scenario library suite validation failed: {suite_errors}")
    
    # Create study directory
    study_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate candidates
    print("[1/6] Generating candidates...")
    if smoke:
        candidates = generate_smoke_candidates()[:trials]
    else:
        candidates = generate_candidates(trials, seed)
    
    print(f"  Generated {len(candidates)} candidates")
    
    # Evaluate all candidates
    print(f"\n[2/6] Evaluating candidates...")
    print(f"  Statistical confidence level: alpha={budgets.alpha}")
    results: List[TrialResult] = []
    
    for i, config in enumerate(candidates):
        trial_id = f"trial_{i:04d}_{config.candidate_id}"
        result = evaluate_trial(
            config=config,
            study_name=study_name,
            trial_id=trial_id,
            study_dir=study_dir,
            mc_runs=mc_runs,
            mc_ticks=mc_ticks,
            seed=seed + i,  # Offset seed per trial for variety
            verbose=verbose,
            adr_config=adr_config,
            alpha=budgets.alpha,
            scenario_library_config=scenario_library_config,
        )
        results.append(result)
    
    # Compute Pareto frontier
    print(f"\n[3/6] Computing Pareto frontier...")
    pareto = compute_pareto_frontier(results)
    print(f"  Pareto size: {len(pareto)}")
    
    for i, r in enumerate(pareto):
        print(f"    {i+1}. {r.candidate_id}: pnl_mean={r.mc_mean_pnl:.2f} (lcb={r.mc_pnl_lcb:.2f}), "
              f"kill_rate={r.mc_kill_prob_point:.3f} (ucb={r.mc_kill_ucb:.3f}), dd_cvar={r.mc_drawdown_cvar:.2f}")
    
    # Write outputs
    print(f"\n[4/6] Writing outputs...")
    
    trials_jsonl = study_dir / "trials.jsonl"
    write_trials_jsonl(results, trials_jsonl)
    print(f"  ✓ {trials_jsonl}")
    
    pareto_json = study_dir / "pareto.json"
    write_pareto_json(pareto, pareto_json)
    print(f"  ✓ {pareto_json}")
    
    pareto_csv = study_dir / "pareto.csv"
    write_pareto_csv(pareto, pareto_csv)
    print(f"  ✓ {pareto_csv}")
    
    # Promote winners
    print(f"\n[5/6] Promoting winners...")
    promotions: Dict[str, Optional[Tuple[Path, Path]]] = {}
    
    for tier_name, budget in sorted(budgets.tiers.items()):
        winner = select_winner(results, budget)
        
        if winner is None:
            print(f"  {tier_name}: No candidate meets budget")
            promotions[tier_name] = None
            continue
        
        print(f"  {tier_name}: Candidate {winner.candidate_id}")
        
        # Verify evidence before promotion
        if not winner.evidence_verified:
            print(f"    ✗ Evidence verification failed - skipping")
            promotions[tier_name] = None
            continue
        
        # Phase B: Confidence-aware statistical gating
        if phase_b_config and phase_b_config.enabled:
            phase_b_result = run_phase_b_gate(
                winner=winner,
                study_dir=study_dir,
                phase_b_config=phase_b_config,
                verbose=verbose,
            )
            
            if not phase_b_result.passed:
                print(f"    ✗ Phase B gate failed - skipping promotion")
                promotions[tier_name] = None
                continue
        
        # Compute seed schedule for provenance
        seed_schedule = [seed + i for i in range(trials)]
        
        config_path, record_path = promote_winner(
            winner, tier_name, study_name, 
            verbose=verbose,
            alpha=budgets.alpha,
            seed_schedule=seed_schedule,
        )
        promotions[tier_name] = (config_path, record_path)
        print(f"    ✓ Promoted to {config_path.name}")
    
    # Summary
    print("\n" + "=" * 70)
    print("Pipeline Complete")
    print("=" * 70)
    print(f"  Total trials: {len(results)}")
    print(f"  Valid trials: {sum(1 for r in results if r.is_valid)}")
    print(f"  Pareto size: {len(pareto)}")
    print(f"  Promotions: {sum(1 for p in promotions.values() if p is not None)}/{len(promotions)}")
    print()
    print(f"Outputs:")
    print(f"  {trials_jsonl}")
    print(f"  {pareto_json}")
    print(f"  {pareto_csv}")
    
    return results, pareto, promotions


# ===========================================================================
# CLI
# ===========================================================================

def main(argv: Optional[List[str]] = None) -> int:
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        prog="python3 -m batch_runs.phase_a.promote_pipeline",
        description="Phase A-2: Multi-objective tuning + promotion pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quick smoke test (3 trials, fast)
  python3 -m batch_runs.phase_a.promote_pipeline --smoke --study-dir runs/phaseA_smoke

  # Run with 10 trials
  python3 -m batch_runs.phase_a.promote_pipeline --trials 10 --study my_study

  # Full research run
  python3 -m batch_runs.phase_a.promote_pipeline --trials 50 --mc-runs 100 --study research_v1

  # With ADR (Adversarial Delta Regression) gating
  python3 -m batch_runs.phase_a.promote_pipeline --smoke --adr-enable --study-dir runs/phaseA_smoke_adr

  # ADR with regression gates
  python3 -m batch_runs.phase_a.promote_pipeline --smoke --adr-enable \\
    --adr-gate-max-regression-usd 50.0 \\
    --adr-gate-max-regression-pct 10.0 \\
    --study-dir runs/phaseA_adr_gated

Outputs:
  - runs/phaseA/<study>/trials.jsonl
  - runs/phaseA/<study>/pareto.json
  - runs/phaseA/<study>/pareto.csv
  - configs/presets/promoted/<tier>/phaseA_<study>_<timestamp>.env
  - configs/presets/promoted/<tier>/PROMOTION_RECORD.json

ADR Outputs (when --adr-enable):
  - <study_dir>/_baseline_cache/<profile>/<suite_name>/  (baseline cache)
  - <trial_dir>/report/<suite_name>.md                   (ADR report)
  - <trial_dir>/report/<suite_name>.json                 (ADR report data)
""",
    )
    
    parser.add_argument(
        "--smoke",
        action="store_true",
        help="Quick smoke mode (3 trials, 10 MC runs, 100 ticks)",
    )
    
    parser.add_argument(
        "--study", "--study-name",
        type=str,
        default=None,
        help="Study name (default: auto-generated timestamp)",
    )
    
    parser.add_argument(
        "--study-dir",
        type=str,
        default=None,
        help="Study directory (overrides default runs/phaseA/<study>)",
    )
    
    parser.add_argument(
        "--trials", "-n",
        type=int,
        default=10,
        help="Number of candidate trials (default: 10)",
    )
    
    parser.add_argument(
        "--mc-runs",
        type=int,
        default=50,
        help="Monte Carlo runs per candidate (default: 50)",
    )
    
    parser.add_argument(
        "--mc-ticks",
        type=int,
        default=600,
        help="Ticks per Monte Carlo run (default: 600)",
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for determinism (default: 42)",
    )
    
    parser.add_argument(
        "--budgets",
        type=str,
        default=None,
        help=f"Path to budgets.yaml (default: {DEFAULT_BUDGETS_FILE})",
    )
    
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress verbose output",
    )
    
    # ADR (Adversarial Delta Regression) options
    parser.add_argument(
        "--adr-enable",
        action="store_true",
        help="Enable ADR (Adversarial Delta Regression) gating",
    )
    
    parser.add_argument(
        "--adr-suite",
        type=str,
        action="append",
        default=[],
        dest="adr_suites",
        metavar="PATH",
        help="Suite path for ADR (repeatable; if omitted, uses default suites)",
    )
    
    parser.add_argument(
        "--adr-baseline-cache",
        type=str,
        default=None,
        metavar="DIR",
        help="Baseline cache directory (default: <study_dir>/_baseline_cache/)",
    )
    
    parser.add_argument(
        "--adr-gate-max-regression-usd",
        type=float,
        default=None,
        metavar="N",
        help="Maximum allowed regression in USD (optional gate)",
    )
    
    parser.add_argument(
        "--adr-gate-max-regression-pct",
        type=float,
        default=None,
        metavar="N",
        help="Maximum allowed regression as percentage (optional gate)",
    )
    
    parser.add_argument(
        "--adr-write-md",
        action="store_true",
        default=True,
        help="Write ADR report in Markdown format (default: on)",
    )
    
    parser.add_argument(
        "--adr-no-write-md",
        action="store_false",
        dest="adr_write_md",
        help="Disable Markdown report output",
    )
    
    parser.add_argument(
        "--adr-write-json",
        action="store_true",
        default=True,
        help="Write ADR report in JSON format (default: on)",
    )
    
    parser.add_argument(
        "--adr-no-write-json",
        action="store_false",
        dest="adr_write_json",
        help="Disable JSON report output",
    )
    
    # Phase B (Confidence-Aware Statistical Gating) options
    parser.add_argument(
        "--phase-b-enable",
        action="store_true",
        help="Enable Phase B confidence-aware statistical gating",
    )
    
    parser.add_argument(
        "--phase-b-alpha",
        type=float,
        default=0.05,
        metavar="ALPHA",
        help="Significance level for Phase B confidence intervals (default: 0.05)",
    )
    
    parser.add_argument(
        "--phase-b-n-bootstrap",
        type=int,
        default=1000,
        metavar="N",
        help="Number of bootstrap samples for Phase B (default: 1000)",
    )
    
    parser.add_argument(
        "--phase-b-seed",
        type=int,
        default=42,
        metavar="SEED",
        help="Random seed for Phase B bootstrap (default: 42)",
    )
    
    parser.add_argument(
        "--phase-b-kill-threshold",
        type=float,
        default=None,
        metavar="RATE",
        help="Maximum allowed kill rate for Phase B (optional)",
    )
    
    parser.add_argument(
        "--phase-b-baseline",
        type=str,
        default=None,
        metavar="PATH",
        help="Path to baseline run directory for Phase B comparison (optional)",
    )
    
    parser.add_argument(
        "--phase-b-no-strict-dominance",
        action="store_true",
        help="Don't require strict dominance (only non-inferiority) for Phase B",
    )
    
    # Scenario Library v1 options (promotion-critical)
    parser.add_argument(
        "--skip-scenario-library",
        action="store_true",
        default=False,
        help="Skip scenario library suite (INSTITUTIONAL EXCEPTION - promotion artifacts may not meet full OOS evaluation)",
    )
    
    parser.add_argument(
        "--scenario-library-suite",
        type=str,
        default=None,
        metavar="PATH",
        help="Override scenario library suite path (default: auto-select based on --smoke mode)",
    )
    
    args = parser.parse_args(argv)
    
    # Handle smoke mode
    if args.smoke:
        trials = min(args.trials, 3)
        mc_runs = 10
        mc_ticks = 100
    else:
        trials = args.trials
        mc_runs = args.mc_runs
        mc_ticks = args.mc_ticks
    
    # Determine study name and directory
    if args.study:
        study_name = args.study
    else:
        study_name = f"study_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if args.study_dir:
        study_dir = Path(args.study_dir)
    else:
        study_dir = DEFAULT_RUNS_DIR / "phaseA" / study_name
    
    # Load budgets
    budgets_file = Path(args.budgets) if args.budgets else DEFAULT_BUDGETS_FILE
    budgets = load_budgets_yaml(budgets_file)
    
    # Build ADR config
    adr_config: Optional[ADRConfig] = None
    if args.adr_enable:
        adr_suites = [Path(s) for s in args.adr_suites] if args.adr_suites else []
        adr_config = ADRConfig(
            enabled=True,
            suites=adr_suites,
            baseline_cache_dir=Path(args.adr_baseline_cache) if args.adr_baseline_cache else None,
            gate_max_regression_usd=args.adr_gate_max_regression_usd,
            gate_max_regression_pct=args.adr_gate_max_regression_pct,
            write_md=args.adr_write_md,
            write_json=args.adr_write_json,
        )
    
    # Build Phase B config
    phase_b_config: Optional[PhaseBConfig] = None
    if args.phase_b_enable:
        phase_b_config = PhaseBConfig(
            enabled=True,
            alpha=args.phase_b_alpha,
            n_bootstrap=args.phase_b_n_bootstrap,
            seed=args.phase_b_seed,
            kill_threshold=args.phase_b_kill_threshold,
            require_strict_dominance=not args.phase_b_no_strict_dominance,
            baseline_run_dir=Path(args.phase_b_baseline) if args.phase_b_baseline else None,
        )
    
    # Build Scenario Library config (ENABLED BY DEFAULT)
    scenario_library_config = ScenarioLibraryConfig(
        enabled=not args.skip_scenario_library,
        suite_path=Path(args.scenario_library_suite) if args.scenario_library_suite else None,
        smoke_mode=args.smoke,
    )
    
    # Print warning if scenario library is skipped
    if args.skip_scenario_library:
        print("=" * 70)
        print("⚠ WARNING: SCENARIO LIBRARY SKIPPED (--skip-scenario-library)")
        print("=" * 70)
        print("This is an INSTITUTIONAL EXCEPTION. Promotion artifacts may not")
        print("meet full out-of-sample evaluation requirements.")
        print("=" * 70)
        print()
    
    # Run pipeline
    try:
        results, pareto, promotions = run_pipeline(
            study_name=study_name,
            study_dir=study_dir,
            trials=trials,
            mc_runs=mc_runs,
            mc_ticks=mc_ticks,
            seed=args.seed,
            budgets=budgets,
            smoke=args.smoke,
            verbose=not args.quiet,
            adr_config=adr_config,
            phase_b_config=phase_b_config,
            scenario_library_config=scenario_library_config,
        )
    except Exception as e:
        print(f"\nERROR: Pipeline failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    # Write root evidence pack for the study directory
    print(f"\n[Evidence Pack]")
    evidence_pack_result = _write_evidence_pack(study_dir, verbose=not args.quiet, smoke=args.smoke)
    if evidence_pack_result != 0:
        print(f"  WARNING: Evidence pack generation failed (exit code {evidence_pack_result})")
    else:
        print(f"  ✓ Evidence pack written to: {study_dir / 'evidence_pack'}")
        # In smoke mode, verify immediately
        if args.smoke:
            verify_result = _verify_evidence_pack(study_dir, verbose=not args.quiet)
            if verify_result != 0:
                print(f"  WARNING: Evidence pack verification failed (exit code {verify_result})")
            else:
                print(f"  ✓ Evidence pack verified")
    
    # Return success if at least one promotion succeeded or pareto exists
    if pareto or any(p is not None for p in promotions.values()):
        return 0
    elif not results:
        return 1
    else:
        return 0


if __name__ == "__main__":
    sys.exit(main())

